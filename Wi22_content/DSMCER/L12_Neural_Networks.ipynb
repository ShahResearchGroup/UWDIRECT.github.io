{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Methods for Clean Energy Research \n",
    "## _Neural Networks_\n",
    "\n",
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dataset & rescale\n",
    "### 2. Build a NN model with Keras\n",
    "### 3. Train NN and test\n",
    "### 4. Increase complexity of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting libraries\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "print(keras.__version__)\n",
    "import numpy as np\n",
    "\n",
    "print(np.__version__)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the data, splitting into train and test and rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set our random seed so that all computations are deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the raw data for the HCEPDB into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://faculty.washington.edu/dacb/HCEPDB_moldata.zip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smaller = df.sample(frac=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smaller.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate out the input features from the output target `'pce'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_smaller[['mass', 'voc', 'jsc', 'e_homo_alpha', 'e_gap_alpha', \n",
    "        'e_lumo_alpha']].values\n",
    "Y = df_smaller[['pce']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the test / train split from this data and keep 20% for testing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pn, X_test_pn, y_train, y_test = train_test_split(X, Y,\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need rescale the input features of the training set using the `StandardScaler()` class [more info](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and store the scaler to use it for the future test sets. We are rescaling input features because we do not want one of the input features to matter more than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scaler from the training data only and keep it for later use\n",
    "X_train_scaler = StandardScaler().fit(X_train_pn)\n",
    "\n",
    "# Apply the scaler transform to the training data\n",
    "X_train = X_train_scaler.transform(X_train_pn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reuse that scaler transform on the test set.  This way we never contaminate the test data with the training data.  We'll start with a histogram of the testing data just to prove to ourselves it is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(10,4))\n",
    "ax[0].hist(X_test_pn[:,0], alpha=0.6, color='mediumvioletred')\n",
    "ax[0].set_xlabel('mass')\n",
    "ax[0].set_ylabel('count')\n",
    "ax[0].set_title('mass distribution',fontsize=18)\n",
    "ax[1].hist(X_test_pn[:,1], alpha=0.6, color='royalblue')\n",
    "ax[1].set_xlabel('voc')\n",
    "ax[1].set_ylabel('count')\n",
    "ax[1].set_title('voc distribution',fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now apply the training scaler transform to the test and plot a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_train_scaler.transform(X_test_pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(10,4))\n",
    "ax[0].hist(X_test[:,0], alpha=0.6, color='mediumvioletred')\n",
    "ax[0].set_xlabel('mass')\n",
    "ax[0].set_ylabel('count')\n",
    "ax[0].set_title('mass distribution',fontsize=18)\n",
    "ax[1].hist(X_test[:,1], alpha=0.6, color='royalblue')\n",
    "ax[1].set_xlabel('voc')\n",
    "ax[1].set_ylabel('count')\n",
    "ax[1].set_title('voc distribution',fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating the neural network model\n",
    "\n",
    "This is a simple neural network with one hidden layer and one output layer. Here we will use `Keras` functions [Keras documentation](https://keras.io/guides/). We will use `Dense` layers as defined [here](https://keras.io/api/layers/core_layers/dense/), and the `Adam` [optimizer](https://keras.io/api/optimizers/adam/) which relies on gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_network():\n",
    "    # Define a sequential model object \n",
    "\n",
    "    # Add a layer to your model - Note - THIS IS YOUR FIRST HIDDEN LAYER - input layer is defined by input_dim!\n",
    "    # Use kernel_initializer='normal' and  activation='relu'\n",
    "\n",
    "    # Add another layer to your model - this is the output layer\n",
    "\n",
    "    # Compile your model using the mean_squared_error for your loss and the adam optimizer\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your simple_network() function\n",
    "\n",
    "# Print it's summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the neural network with the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the numpy random number generator - i.e. get reproducible starting weights \n",
    "\n",
    "\n",
    "# Create the NN estimator with the KerasRegressor and your simple_network function, \n",
    "# use 150 epochs and a batch_size of 10000\n",
    "\n",
    "\n",
    "# Fit your estimator to your *** data with a validation_split of 0.3 and the same \n",
    "# epochs and batch size as above save results in an object you call \"history\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history object returned by the `fit` call contains the information in a fitting run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"final MSE for train is %.2f and for validation is %.2f\" % \n",
    "      (history.history['loss'][-1], history.history['val_loss'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of model loss\n",
    "plt.plot(history.history['loss'], '--', c='b')\n",
    "plt.plot(history.history['val_loss'], c='crimson')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the MSE for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set error\n",
    "test_loss = estimator.model.evaluate(X_test, y_test)\n",
    "print(\"test set mse is %.2f\" % test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our train mse is **very similar** to the training and validation at the final step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Test the following\n",
    "\n",
    "* 1) Change the **number of neurons in each layer** - do you get any errors?\n",
    "* 2) Change the **optimizer** - see [here](https://keras.io/api/optimizers/) for a list of optimzers in Keras\n",
    "* 3) Change the **activation functions** of both the hidden layer and the output layer, does your cost function final value change?\n",
    "* 4) Test the model by prediction the **unscaled `X_test_pn`**, how does the loss change? \n",
    "* 5) Train the model with the **unscaled input features** `X_train_pn` - how does the loss / cost function change?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new function which now takes activation functions in input\n",
    "def simple_network_2(n1, n2, act1, act2):\n",
    "    # Define the sequential model\n",
    "    \n",
    "    # Add first hidden layer and use activation=act1\n",
    "\n",
    "    # Add second hidden layer and use activation=act2\n",
    "\n",
    "    # Compile the model as we had done before\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset your random number generator\n",
    "\n",
    "\n",
    "# Create your NN estimator using Keras Regressor - specify your two activation functions\n",
    "\n",
    "\n",
    "# Fit your estimator to your *** data with the same validation split epochs etc as before - store the results in a variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Let's look at another way to evaluate the set of models using cross validation\n",
    "\n",
    "Use 10 fold cross validation to evaluate the models generated from our training set.  We'll use scikit-learn's tools for this.  Remember, this is only assessing our training set.  If you get negative values, to make `cross_val_score` behave as expected, we have to flip the signs on the results (incompatibility with keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "\n",
    "results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (-1 * results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Increase complexity of NN\n",
    "\n",
    "Let's add a second hidden layer this time. Note: it is worthwhile to test how the final loss changes with number of epochs of training as well as learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medium_network():\n",
    " \n",
    "    model = Sequential()\n",
    "    model.add(Dense(600, input_dim=6, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(120, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    opt = keras.optimizers.Adam(learning_rate=0.9)\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = medium_network()\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(seed)\n",
    "# Create the NN framework\n",
    "estimator = KerasRegressor(build_fn=medium_network,\n",
    "        epochs=150, batch_size=10000, verbose=0)\n",
    "# Fit to training data\n",
    "history = estimator.fit(X_train, y_train, validation_split=0.30, epochs=150, \n",
    "        batch_size=10000, verbose=0)\n",
    "\n",
    "print(\"Final MSE for train is %.3e and for validation is %.3e\" % \n",
    "      (history.history['loss'][-1], history.history['val_loss'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize history for loss\n",
    "plt.plot(history.history['loss'],'--',c='b')\n",
    "plt.plot(history.history['val_loss'],c='crimson')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "#plt.ylim([0,100])\n",
    "plt.xlim([0,50])\n",
    "plt.ylim([0,20])\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = estimator.model.evaluate(X_test, y_test)\n",
    "print(\"test set mse is %.3e\" % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medium_network(lr=0.8):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, input_dim=6, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(12, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "MSE = []\n",
    "lr_vals = [0.00001, 0.0001, 0.01, 0.1, 1.0]\n",
    "for lrate in lr_vals:\n",
    "    estimator = KerasRegressor(build_fn=medium_network, \n",
    "                               epochs=150, batch_size=10000, \n",
    "                               verbose=0, lr=lrate)\n",
    "    history = estimator.fit(X_train, y_train, validation_split=0.30,\n",
    "                            epochs=150, batch_size=10000, \n",
    "                            verbose=0)\n",
    "    print(\"Final MSE for train is %.3e and for validation is %.3e\" % \n",
    "      (history.history['loss'][-1], history.history['val_loss'][-1]))\n",
    "    MSE.append([history.history['loss'][-1], \n",
    "                history.history['val_loss'][-1],\n",
    "                estimator.model.evaluate(X_test, y_test)])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lr_vals, [m[0] for m in MSE],'-o',lw=3,label='train')\n",
    "plt.plot(lr_vals, [m[1] for m in MSE],'-.s',lw=3, label='validation')\n",
    "plt.plot(lr_vals, [m[2] for m in MSE],':>',lw=3, label='test')  \n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
