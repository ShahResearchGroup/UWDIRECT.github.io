{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Methods for Clean Energy Research \n",
    "## _Unsupervised Machine Learning_\n",
    "\n",
    "## Outline\n",
    "\n",
    "### 1. Principal Component Analysis\n",
    "### 2. K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "\n",
    "import pandas as pd \n",
    "import random\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Principal Component Analysis\n",
    "\n",
    "### 1.1 What is PCA?\n",
    "\n",
    "PCA finds a low-dimensional representation of a high-dimensional dataset \n",
    "\n",
    "$$X = \\left\\{X_1,X_2,...,X_p\\right\\}$$\n",
    "\n",
    "which contains as much of the information as possible on the high-dimensional dataset. \n",
    "\n",
    "The low-dimensional representation will have dimensions $n<p$ where $p$ the number of input features also represents the dimensions of the dataset to some extent.\n",
    "\n",
    "* Each of the $n$ low-dimensional components can be written as a linear combination of the input features\n",
    "* All low-dimensional components are uncorrelated which also means, in this case, that they are orthogonal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some **applications of PCA:**\n",
    "\n",
    "* Reducing dimensionality\n",
    "* Accelerating ML methods ( finding optimal features )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **first** principal component of our set of features $X = \\left\\{X_1,X_2,...,X_p\\right\\}$ is the linear combination\n",
    "\n",
    "$$Z_1 = \\phi_{11}X_1 \\,+\\, \\phi_{21}X_2 \\,+\\, ...\\,+\\,\\phi_{p1}X_p$$\n",
    "\n",
    "which has the largest variance. The coefficients $\\left\\{\\phi_{1,i}\\right\\}_{i=1}^{p}$ are called _loadings_ of the first principal component. The _loadings_ are such that $Z_1$ is normalized, i.e. $\\sum_i^p\\phi_{i,1}^2=1$\n",
    "\n",
    "The **second** principal component is the direction which maximizes variance among all directions orthogonal to the first. \n",
    "\n",
    "The $k$-th component is the variance-maximizing direction orthogonal to the previous $k âˆ’ 1$ components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The PCA optimization problem\n",
    "\n",
    "**How do we find these _loadings_** with the constraint of largest variance?\n",
    "\n",
    "Let's say you are given a data set $X$ of $n_{pts}$ examples, i.e. of size $n_{pts}\\times p$, and want to find\n",
    "\n",
    "$$z_{i1} = \\phi_{11}x_{i1} + \\phi_{21}x_{i2} + ...\\,+\\,\\phi_{p1}x_{ip}\\;\\;\\text{with } i\\in\\left[1,n\\right]$$\n",
    "\n",
    "that has the largest sample variance.\n",
    "\n",
    "One can show, e.g for the first component that the solution is to solve the optimization problem\n",
    "\n",
    "$$\\underset{\\phi_{11},...\\phi_{p1}}{\\text{maximize}}\\left\\{\\frac{1}{n_{pts}}\\sum_{j=1}^{n_{pts}}{z_{j1}^2}\\right\\}\\;\\text{ subject to}\\sum_i^p\\phi_{i,1}^2=1$$\n",
    "\n",
    "the process is similar for all subsequent components. \n",
    "\n",
    "**Note** that the above equation assumes that the means of each $X_i$ is **equal to zero**. \n",
    "\n",
    "Also, we usually **rescale** the $X_i$ to have a variance equal to 1. This insures that no feature will dominate over another due to a difference in range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 How to implement PCA using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some data using the [`RandomState`](https://numpy.org/doc/1.16/reference/generated/numpy.random.RandomState.html) function. We want 3 columns and 200 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "X = np.dot(rng.rand(3, 3)*0.5, rng.randn(3, 200)*10).T # Dot product between uniformly sampled points and gaussian samples points\n",
    "\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we have three input features but are only going to plot the data\n",
    "# as a function of the first two input columns\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1],color='darkseagreen')\n",
    "plt.xlabel('$X_1$')\n",
    "plt.ylabel('$X_2$')\n",
    "plt.legend(['Dataset'])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement the PCA algorithm and search for $n=3$ principal components which are orthogonal and best represent our dataset. Have a look at the scikit-learn [`PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) function and test it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PCA object for 3 components\n",
    "\n",
    "\n",
    "# Fit the PCA object to ... ?\n",
    "\n",
    "\n",
    "# Print the PCA components and explained variance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that your components are normalized to 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the above for `n_components=2` do the PCAs change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Proportion of Variance explained\n",
    "\n",
    "What is the **proportion of variance explained (PVE)**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a plot of the explained variance (y-axis) as a function of the principal components (x-axis). What trend do you see? What does it show you about your principal components? For the principal components we can use [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1,2,3], pca.explained_variance_ratio_,':>')\n",
    "plt.xlabel('principal component')\n",
    "plt.ylabel('PVE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PVE tells us how much of the total variance in the $X$ data is explained by each component! The larger it is the more it describes the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here it is clear that we might be ok with simply taking two input features to represent our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time to plot the components!**\n",
    "\n",
    "Let's go back to two input features and look at the PCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating new data\n",
    "rng = np.random.RandomState(seed=42)\n",
    "X_2 = np.dot(rng.rand(2, 2)*10, rng.randn(2, 200)*2).T\n",
    "\n",
    "# Calling PCA \n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_2.fit(X_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function to draw two vectors\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, color='purple')\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can scatter plot the data and plot the vectors\n",
    "plt.scatter(X_2[:, 0], X_2[:, 1], alpha=0.2, color='mediumorchid')\n",
    "plt.xlabel('$X_1$')\n",
    "plt.ylabel('$X_2$')\n",
    "for length, vector in zip(pca_2.explained_variance_, pca_2.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca_2.mean_, pca_2.mean_ + v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `explained_variance_ratio_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA can become **expensive to calculate**, especially as the data set size grows \n",
    "\n",
    "**Advice:**\n",
    "* Go slow and use a subset of your data (if you have many points) \n",
    "* Use PCA as a guide and as an exploratory tool \n",
    "* Constantly interrogate the results and ask if they make sense! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The K-means method and algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While PCA searches for a low dimensional representation of the dataset, Clustering methods search for homogeneous subgroups among the observations.\n",
    "\n",
    "In K-means clustering, we define a desired number of clusters _K_ and assign each observation to one of the clusters.\n",
    "\n",
    "**Rules:** \n",
    "* Each observation must be placed in at least one of the clusters\n",
    "* No clusters may overlap, each observation can only be placed in a single cluster \n",
    "* The goal is to minimize the variance of observations within each of the clusters \n",
    "\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "Choose a values for your number of clusters _K_:\n",
    "\n",
    "   * At random select _K_ points from your dataset and assign those to be *centroids*, cluster centers\n",
    "   * Assign each data point to the closest centroid (use the Euclidean distance)\n",
    "   * Re-compute the centroids using the currently assigned clusters\n",
    "   * If a convergence criterion is not met, repeat steps 2 and  3\n",
    "\n",
    "This algorithm corresponds to solving the problem\n",
    "\n",
    "$$\\underset{C_{1},...,C_{K}}{\\text{minimize}}\\sum_{j=1}^K\\sum_{{\\bf x}\\in {\\bf C}_j}d({\\bf x},{\\bf m}_j)^2$$\n",
    "\n",
    "with \n",
    "\n",
    "$d({\\bf x},{\\bf m}_j)$ the Euclidean distance between data point x\n",
    "and centroid $m_j$, $C_j$ the $j-$th cluster, and $m_j$\n",
    "is the centroid of cluster $C_j$ i.e. the mean vector of all the\n",
    "data points in $C_j$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 K-means from scratch\n",
    "\n",
    "Let's implement it from scratch (following this [tutorial](https://towardsdatascience.com/a-complete-k-mean-clustering-algorithm-from-scratch-in-python-step-by-step-guide-1eb05cdcd461))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will generate data, note that while we are generating $y$ - **the algorithm will never see it or know of its existence (unsupervised!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_features=2, n_samples=970, centers=3, cluster_std=4.0, random_state=98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we randomly select some examples in X as our centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select 3 indices randomly using random.sample\n",
    "init_centroid_indices = random.sample(range(0, len(X)), 3)\n",
    "\n",
    "print(init_centroid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we use the indices to define the values of our centroids in terms of X\n",
    "centroids = []\n",
    "for i in init_centroid_indices:\n",
    "    centroids.append(X[i])\n",
    "centroids = np.array(centroids)\n",
    "\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to compute the Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eucl_dist(X1, X2):\n",
    "    return np.sqrt(sum((X1 - X2)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the `assigned_centroids` function below. It aims to assign the centroid each point X belongs to, based on its distance the points in each centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_centroids(centroids, X):\n",
    "    assigned_centroids = []\n",
    "    for xi in X:\n",
    "        distance = []\n",
    "        for Cj in centroids:\n",
    "            # Complete here - fill out the eucledian distance of each point to the centroid\n",
    "            \n",
    "        # From the distances choose the assigned centroids\n",
    "        \n",
    "    return assigned_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call your function ot assign the centroids and save the assigned centroids\n",
    "\n",
    "\n",
    "# Print your centroids .. what does it all mean?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last function to implement is one to update the centroids based on the mean for features in each assigned centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_centroids(clusters, X):\n",
    "    new_centroids = []\n",
    "    # We define a dataframe which include the X data and the clusters\n",
    "    df = pd.concat([pd.DataFrame(X), pd.DataFrame(clusters, columns=['cluster'])], axis=1)\n",
    "    for c in set(df['cluster']):\n",
    "        # Now we compute mean of each cluster\n",
    "        current_cluster = df[df['cluster'] == c][df.columns[:-1]]\n",
    "        cluster_mean = current_cluster.mean(axis=0)\n",
    "        # And update the position of the centroids based on the mean\n",
    "        new_centroids.append(cluster_mean)\n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this looks like in a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "# For 10 iterations we will\n",
    "for i in range(10):\n",
    "    \n",
    "    # Assign centroids\n",
    "    assigned_centroids = assign_centroids(centroids, X)\n",
    "    \n",
    "    # Update the centroids using the means\n",
    "    centroids = update_centroids(assigned_centroids, X)\n",
    "    \n",
    "    print(\"iteration \", i)\n",
    "    for ct in centroids:\n",
    "        print(str(ct[0]) +\", \"+str(ct[1]) )\n",
    "    \n",
    "    pl.clf()\n",
    "    pl.scatter(X[:, 0], X[:, 1], alpha=0.3, c=y, s=50, cmap='coolwarm')\n",
    "    pl.scatter(np.array(centroids)[:, 0], \n",
    "               np.array(centroids)[:, 1], \n",
    "               marker='>', color='black', s=80)\n",
    "    pl.title('Iteration '+str(i))\n",
    "    pl.xlabel('$X_1$', fontsize=20)\n",
    "    pl.ylabel('$X_2$', fontsize=20)\n",
    "    display.display(pl.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(2.0)\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the centroids\n",
    "for ct in centroids:\n",
    "        print(str(ct[0]) +\", \"+str(ct[1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means with `sklearn` \n",
    "\n",
    "Now let's try the same thing with the sklearn `KMeans` [class](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your K-Means object\n",
    "\n",
    "# Fit your K-Means object to your ?? data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print your labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print your kmeans clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to the method we used above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out with a different number of clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
