{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Methods for Clean Energy Research \n",
    "## *Introduction to machine learning: KNN classification*\n",
    "\n",
    "## Outline\n",
    "\n",
    "Today, we will look at the problem of classification in machine learning. Specifically we will consider the supervised machine learning K-nearest neighbors, KNN, classification method. \n",
    "\n",
    "We will also start looking at the `scikit-learn` library which implements functions for Machine Learning algorithms. Main webpage for scikit-learn can be found [here](https://scikit-learn.org/stable/).\n",
    "\n",
    "### 1. Introduction to KNN\n",
    "* 1.1 The algorithm\n",
    "* 1.2 Nearest points\n",
    "\n",
    "### 2. Example with KNN\n",
    "* 2.1 The iris dataset\n",
    "* 2.2 Fitting a KNN and predicting\n",
    "* 2.3 The role of K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's load the libraries / modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to K-nearest neighbor classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN classifier works on assigning the probability of membership in a class $j$ based on the **distance to other members** in a class. In this process it estimates the conditional probability of $Y = j$ given a test observation $x_{0}$. \n",
    "\n",
    "### 1.1 The KNN algorithm\n",
    "\n",
    "The KNN algorithm can be described as following.\n",
    "\n",
    "You have a **training** data set $D_{\\sf train}$ with _N_ points, a **test observation** $x_{test}=x_0$ and a **set of classes** or categories  $C={C_1, C_2, .., C_{N_{\\sf classes}}}$. You know which class each value in $D_{\\sf train}$ belongs to and desire to know what is the most likely class $x_0$ belongs to - i.e. you want to classify it.\n",
    "\n",
    "* Find $K$ points in $D_{\\sf train}$ which are closest to $x_0$.\n",
    "    * For each $(x, C_x) \\in D_{\\sf train}$ compute the distance between $x$ and $x_0$ (usually Euclidean distances - the L2 norm)\n",
    "    * Sort the distances\n",
    "    * Take the top $K$ nearest points and define that set as $\\mathcal{N}_0$\n",
    "   \n",
    "    \n",
    "* Count the number of points in $\\mathcal{N}_0$ which fall in each class  i.e. we are estimating the conditional probability of $x_0$ belonging to class $j$ as the fraction of points in $\\mathcal{N}_0$ whose response values equals $j$. Mathematically:\n",
    "\n",
    "$${\\sf Pr}\\left(Y = j \\mid X = x_{\\sf{0}}\\right)=\\frac{1}{K}\\sum_{i\\in\\mathcal{N}_0}I(y_i=j)$$ \n",
    "\n",
    "* Note: Sometimes it is convenient to add a weight term which is e.g. inversely proportional to the distance when assigning points to classes. This allows points which are very close to the target to count more than points which are further away. In this case the method is referred to as **weighted KNN**\n",
    "\n",
    "* Choose the class as the one with the largest fraction of points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://miro.medium.com/max/1182/0*oq9T_Rg75IF4bIW_.png\" width=\"400\" align=center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how do we implement this in Python? Luckily for use `scikit-learn` comes to the rescue - https://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Nearest points "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in each dimension of X should be **scaled similarly**! \n",
    "We are concerned with **relative** distances in the parameter space, not absolute distances in the underlying units.\n",
    "\n",
    "Otherwise units, dimensionality, etc. can drastically favor closeness in arbitrary dimensions of X. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Example with KNN's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The iris flower dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the [iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set). For more information see [here](https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py). \n",
    "\n",
    "If you are curious to see how you could code this from scratch see [here](https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the iris dataset into a dictionary like object\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "print(\"keys:\\n\", iris.keys())\n",
    "print(\"\\ninput feature names:\\n\", iris['feature_names'])\n",
    "print(\"\\ntarget class names:\\n\", iris['target_names'])\n",
    "print(\"\\ntarget class values:\\n\", iris['target'])\n",
    "print(iris['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the **training** and **testing** data. Out of simplicity we will only take two input features as our X values. We usually want the training dataset to be larger than the testing dataset ~ between 70-85% is most common. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only take the first two features. \n",
    "X = iris.data[:, :2]  # Training set input - sepal length and width\n",
    "y = iris.target       # Y - our target classes - we define these as Y as \n",
    "                      # because that is the standard notation for a targetin ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set the training data set to be 80% of the data and the test set 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Fitting and predicting classes with KNN in `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting\n",
    "\n",
    "We define a number of nearest neighbors K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_neighbors = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a KNN classifier on the training set - for each input feature we are training it to predict the corresponding class. Here we will use the `scikit-learn` KNN class - for more info see [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "\n",
    "`KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)`\n",
    "\n",
    "* `n_neighbors` is our K value - number of nearest neighbors\n",
    "* `weights='uniform'` indicates that we apply KNN not the distance weighted KNN\n",
    "* by setting `p=2` we use the Euclidean distance. The other parameters are related to other methods to compute distances. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an instance of Neighbours Classifier and fit it to the training data.\n",
    "# if you set weights = 'distance' here you will have a weighted KNN classifier\n",
    "clf = neighbors.KNeighborsClassifier(K_neighbors, weights='uniform')\n",
    "\n",
    "# Here we fit it to the data - it figures out the classes and \n",
    "# which training X points belong to each class in Y \n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.classes_, clf.effective_metric_, clf.n_samples_fit_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting\n",
    "\n",
    "Now we will use the trained classifier to decide which class the test set points belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted = clf.predict(X_test)\n",
    "y_test_probabilities = clf.predict_proba(X_test)\n",
    "mean_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"predicted classes\", y_test_predicted)\n",
    "print(\"KNN probability of belonging to class\",y_test_probabilities)\n",
    "print(\"Mean accuracy of prediction\", mean_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create color maps\n",
    "mycols = ListedColormap(['lightskyblue', 'Salmon', 'Turquoise'])\n",
    "mycols_dark = ListedColormap(['Blue', 'Firebrick', 'Darkcyan'])\n",
    "\n",
    "markers = ['s','o','>']\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=mycols, edgecolor='k', s=80)\n",
    "plt.title('training set')\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('sepal width [cm]')\n",
    "plt.show()\n",
    "\n",
    "classes = ['Iris-setosa', 'Iris-virginica', 'Iris-versicolor']\n",
    "plt.figure()\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=mycols, edgecolor='k', s=80, alpha=1, label = 'test set')\n",
    "ind = 0\n",
    "for y in np.unique(y_test_predicted):\n",
    "    plt.scatter(X_test[y_test_predicted == ind, 0], X_test[y_test_predicted == ind, 1], color=mycols_dark(ind), marker = 'x',  edgecolor='k', s=100, label = classes[y])\n",
    "    ind+=1\n",
    "    \n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.7,0.8))\n",
    "plt.title('test set & predicted classes')\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('sepal width [cm]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another fancy plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = [1, 10, 50, 100, 150]\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# we only take the first two features. We could avoid this ugly\n",
    "# slicing by using a two-dim dataset\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "mycols_dark_sns = ['RoyalBlue', 'Firebrick', 'Darkcyan']\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "for K in n_neighbors:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(K, weights='uniform')\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, cmap=mycols, alpha=0.7)\n",
    "\n",
    "    # Plot also the training points\n",
    "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=iris.target_names[y],\n",
    "                    palette=mycols_dark_sns, alpha=1.0, s=80, style= iris.target_names[y], edgecolor=\"black\")\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification ; K = \"+str(K))\n",
    "    plt.xlabel(iris.feature_names[0])\n",
    "    plt.ylabel(iris.feature_names[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try playing with the value of K - what do you see changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
