{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhMpsTQGPQIm"
   },
   "source": [
    "# Data Science Methods for Clean Energy Research \n",
    "## _Multiple linear regression_\n",
    "\n",
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXXn9xy52KPj"
   },
   "source": [
    "Today we will move beyond the simple one independent variable **linear regression** and look at how to choose, train and predict using **multiple linear regression**. \n",
    "\n",
    "### 1. Introduction to multiple linear regression\n",
    "#### 1.1 Multiple linear regression\n",
    "#### 1.2 Let's run this in Python for a new dataset - the diabetes dataset \n",
    "#### 1.3 Accuracy of multiple linear regression fit\n",
    "#### 1.4 Feature selection\n",
    "\n",
    "* 1.4.1 _F-statistic_ to estimate relationship between response and predictors\n",
    "* 1.4.2 Forward and Backward stepwise selection\n",
    "\n",
    "\n",
    "#### 1.5 Potential issues with linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2fOL1IBhB-o"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jc0SbuFeQBwW"
   },
   "source": [
    "### Load libraries which will be needed in this Notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1610125350172,
     "user": {
      "displayName": "Stephanie Valleau",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPxYC_xbIb6qmkXo8iMld6AvpYL5dO54osIaSx=s64",
      "userId": "13099634571785749992"
     },
     "user_tz": 480
    },
    "id": "XA8E1GTQQBwW",
    "outputId": "a0e03050-f4d1-4ad4-893c-079b81cd9097"
   },
   "outputs": [],
   "source": [
    "# Pandas library for the pandas dataframes\n",
    "import pandas as pd    \n",
    "\n",
    "# Import Scikit-Learn library for the regression models\n",
    "import sklearn         \n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Note - you will need version 0.24.1 of scikit-learn to load this library (SequentialFeatureSelector)\n",
    "from sklearn.feature_selection import f_regression, SequentialFeatureSelector\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Import numpy \n",
    "import numpy as np\n",
    "\n",
    "# Another statistic model library\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "from scipy import interpolate\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Import plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "# Set larger fontsize for all plots\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Command to automatically reload modules before executing cells\n",
    "# not needed here but might be if you are writing your own library \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cm6isl1eQBwX"
   },
   "source": [
    "## 1. What is mutiple linear regression? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from last time. We were fitting the position of the Cheetah as a function of time only. However, the **speed** or velocity of the Cheetah (the dependent variable v) could depend on:\n",
    "\n",
    "* his/her **energy** (independent variable E) that day\n",
    "* how well the Cheetah **slept** the night before (independent variable sleep, S) \n",
    "* whether she/he was well **fed** (independent variable, F) \n",
    "* how much he or she has been **training** recently (independent variable, T)\n",
    "\n",
    "Each of these could be considered as an input feature _X_ ... and perhaps the position depends on many or all of these features ... How do we take this into account?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Multiple linear regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crBj3b4FHemQ"
   },
   "source": [
    "It is the process of predicting a **_dependent_** variable _Y_ based on <span style=\"color:blue\"> **more than one** </span> **_independent_** variable $X_1$, $X_2$, ..., $X_p$ with $p$ the number of parameters using a multiple linear regression model:\n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon\\;\\;\\;\\;\\;\\sf{eq. 1}$$ \n",
    "\n",
    "\n",
    "The $\\left\\{\\beta_i\\right\\}_{i=0}^{p}$ **_coefficients_** / **_parameters_** are estimated using least squares, as we did for simple linear regression\n",
    "\n",
    "\n",
    "Specifically we need to minimize the **residual sum of squares** (RSS) which for  $N$ points $\\left\\{(x_{0,1},x_{0,2},...,x_{0,p},y_0),...,(x_{N,1},x_{N,2},...,x_{N,p},y_0)\\right\\}$ can be written as\n",
    "\n",
    "$${\\sf RSS} =\\sum_{i=1}^{N}\\left(y_i - \\hat{y}_i\\right)^2 = \\sum_{i=1}^{N}\\left(y_i - \\hat{\\beta}_0 -\\hat{\\beta}_1 x_{i,1} -\\hat{\\beta}_2 x_{i,2} \\;-\\;...\\;-\\;\\hat{\\beta}_p x_{i,p}\\right)^2\\;\\;\\;\\;\\sf{eq. 2}$$\n",
    "\n",
    "The optimal values -i.e. the parameter estimators $\\left\\{\\hat{\\beta}_i\\right\\}_{i=0}^{p}$  can be represented using matrix algebra. If you are interested you can find them on [wikipedia](https://en.wikipedia.org/wiki/Linear_regression#Simple_and_multiple_linear_regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Let's run this in Python for a new dataset - the diabetes dataset \n",
    "\n",
    "For more information on this dataset see [here](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = datasets.load_diabetes()\n",
    "\n",
    "X, y = dset.data, dset.target\n",
    "print(dset.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In class exercise\n",
    "\n",
    "Use the values of X and y and the `scikit-learn` [`linear_model.LinearRegression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class to fit a multilinear regression model. Use the first 3 input features.\n",
    "\n",
    "* Define a train and test set\n",
    "* Fit the model on the ... set\n",
    "* Predict the values of y for the ... set\n",
    "* Plot predicted respect to fitted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKfamij9QBwb"
   },
   "source": [
    "### 1.3 Accuracy of multiple linear regression fit\n",
    "\n",
    "We can look at the residual standard error, RSE which for multiple linear regression has the expression:\n",
    "\n",
    "$$\\sigma \\equiv \\text{RSE}= \\sqrt{\\frac{\\text{RSS}}{N-p -1}}\\;\\;\\;\\;\\;\\sf{eq. 6}$$\n",
    "\n",
    "with RSS the residual sum of squares, i.e.\n",
    "\n",
    "$$\\text{RSS} = \\sum_{i=1}^{N} \\left(y_i - \\hat{y}_i\\right)^2\\;\\;\\;\\;\\;\\sf{eq. 7}$$\n",
    "\n",
    "we can normalize the RSS by the number of points N to retrieve the MSE as a metric of error of our fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean squared error\n",
    "print('Mean squared error: %.2f' % mean_squared_error(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5m88vMlkQBwb"
   },
   "source": [
    "We can also look at the regression score $R^2$ as we did for simple linear regression\n",
    "\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}}\\;\\;\\;\\;\\sf{eq. 8}$$\n",
    "\n",
    "with \n",
    "\n",
    "$$\\text{TSS}=\\sum_{i=1}^N \\left(y_i-\\bar{y}\\right)^2\\;\\;\\;\\;\\sf{eq. 9}$$\n",
    "\n",
    "The best value of $R^2$ is 1 but it can also take a negative value if the error is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficient of determination: %.2f' % r2_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSsBFNvcQBwb"
   },
   "source": [
    "Our error and coefficient of determination indicate that we are not doing a great job in predicting .. \n",
    "\n",
    "* Do we need more input features? \n",
    "* How are our independent variables correlated to our target?\n",
    "* Which input features should we choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Feature selection\n",
    "\n",
    "### 1.4.1 _F-statistic_ to estimate relationship between response and predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this was simple linear regression our alternative hypothesis would be - the target value depends on the slope $\\beta_1$ and the null hypothesis would be - the target values does not depend on the slope.\n",
    "\n",
    "Let's extend this to multilinear regression\n",
    "\n",
    "We hypothesize\n",
    "* Null hypothesis $H_0:\\;\\beta_1=\\beta_2=...=\\beta_p=0$ there is no dependence on any input feature\n",
    "* Alternative hypothesis $H_a:$ at least one $\\beta_j\\neq0$ it depends at least on one input feature\n",
    "\n",
    "We can verify this hypothesis using the so-called $F-statistic$ (more info [here](https://en.wikipedia.org/wiki/F-test) and also [here](https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/feature_selection/_univariate_selection.py#L232))\n",
    "\n",
    "$$F=\\frac{(\\text{TSS}-\\text{RSS})/p}{\\text{RSS}/(N-p-1)}\\;\\;\\;\\;\\sf{eq. 3}$$\n",
    "$$\\text{RSS} = \\sum_{i=1}^{N} \\left(y_i - \\hat{y}(x_i)\\right)^2\\;\\;\\;\\;\\;\\sf{eq. 4}$$\n",
    "\n",
    "$$\\text{TSS}=\\sum_{i=1}^N \\left(y_i-\\bar{y}\\right)^2\\;\\;\\;\\;\\sf{eq. 5}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the linear model assumptions are correct - i.e. the alternative hypothesis is correct then one would expect $F$ to be greater than 1. \n",
    "\n",
    "One can also iteratively test each input feature or sets of input features, the expression is slightly different respect to the equation above. Let's do this in python using the `scipy` `OLS()` [function](https://www.statsmodels.org/stable/_modules/statsmodels/stats/contrast.html#ContrastResults.summary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mreg = sm.OLS(y_train, X_train).fit()\n",
    "mreg.summary(alpha=0.1) # Set significance level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the F value is larger than one and the p-value less than $\\alpha$ therefore we can reject the null hypothesis. This means that there is some correlation between our input features and target output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Forward and Backward stepwise selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which are the best input features - independent variables? Should we test every possible combination?\n",
    "\n",
    "One class of methods to find the best input features is **Subset Selection**. Here the approaches aim to indentify a subset of $p$ predictions which are related to the response $y$ and fitting a model using least squares.\n",
    "\n",
    "Two other important classes of methods include **Shrinkage** and **Dimension Reduction**. For more information see Chapter 6 section 6.1 of the textbook.\n",
    "\n",
    "We will look at two **Stepwise Selection** methods from the first class:\n",
    "\n",
    "* Forward Stepwise Selection\n",
    "* Backward Stepwise Selection\n",
    "\n",
    "Let's see how to do this in scikit-learn [`SequentialFeatureSelector()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward stepwise selection\n",
    "\n",
    "Foward stepwise selection starts with a model with **no** predictors, $\\beta$ and then at each step it adds the input variable which leasds to the largest improvement to the fit\n",
    "\n",
    "**_step 1_** Start with $\\mathcal{M}_0$ the _null_ model with no predictors\n",
    "\n",
    "**_step 2_** For $k=0,...,p-1$:\n",
    "  * Consider all models with $p-k$ parameters which increase the number of predictors in $\\mathcal{M}_k$ by one\n",
    "  * Choose the best of the above set of models (with the smallest RSS or highest $R^2$ score - we will call it $\\mathcal{M}_{k+1}$\n",
    "\n",
    "**_step 3_** Select the best among these models {$\\mathcal{M}_0$,...,$\\mathcal{M}_p$} using cross-validated prediction error (we will see cross-validation in the next lecture)\n",
    "\n",
    "Note: You can use forward feature selection even when $N<p$ but you will only be able to evaluate submodels up to $p=N-1$, for larger values of $p$ the solution is not unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find features which are most important\n",
    "\n",
    "# sfs_forward = SequentialFeatureSelector()\n",
    "\n",
    "# Print features which are most important\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward stepwise selection\n",
    "\n",
    "Backward stepwise selection starts with a model with **all** predictors, $\\beta$ and then at each step it adds the input variable which leasds to the largest improvement to the fit\n",
    "\n",
    "**_step 1_** Start with $\\mathcal{M}_p$ the _full_ model with all predictors\n",
    "\n",
    "**_step 2_** For $k=p,p-1,...,1$:\n",
    "  * Consider all models with $k$ parameters which decrease the number of predictors in $\\mathcal{M}_k$ by one\n",
    "  * Choose the best of the above set of models (with the smallest RSS or highest $R^2$ score - we will call it $\\mathcal{M}_{k-1}$\n",
    "\n",
    "**_step 3_** Select the best among these models {$\\mathcal{M}_0$,...,$\\mathcal{M}_p$} using cross-validated prediction error (we will see cross-validation in the next lecture)\n",
    "\n",
    "Note: Here $N>=p$ must be satisfied - otherwise the full model can't be fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sfs_backward = SequentialFeatureSelector(**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbXhulIfQBwc"
   },
   "source": [
    "### 1.5 Potential issues with linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tizRPJ_GQBwc"
   },
   "source": [
    "When fitting data using a linear regression model some issues can occur. Amongst these we will look at the case when the **response-predictor relationship is non-linear**. This can be identified if a pattern is in the residual plot for linear fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "ax[0].scatter(y_test, y_test-y_predict, marker='o')\n",
    "ax[0].set_title('Residual plot for linear fit\\n', fontsize=15)\n",
    "ax[0].set_xlabel('y_test')\n",
    "degree = 5\n",
    "model = make_pipeline(PolynomialFeatures(degree), linear_model.LinearRegression())\n",
    "model.fit(X, y)\n",
    "y_poly = model.predict(X_test)\n",
    "\n",
    "ax[1].scatter(y_test, y_test-y_poly, c=\"purple\")\n",
    "ax[1].set_title('Residual plot for polynomial fit\\n', fontsize=15)\n",
    "ax[1].set_xlabel('y_test')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a pattern in the residuals for the linear fit which looks non linear. This should not be the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other possible problems to keep in mind - let's go back to the slides to see these.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C-HACK Tutorial 5: Regression and Error for instructors.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
