{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhMpsTQGPQIm"
   },
   "source": [
    "# Data Science Methods for Clean Energy Research \n",
    "## _Decision trees, bagging, random forests and boosting_\n",
    "\n",
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXXn9xy52KPj"
   },
   "source": [
    "\n",
    "### 1. Decision trees\n",
    "\n",
    "* 1.1 Regression trees\n",
    "* 1.2 Classification trees\n",
    "\n",
    "\n",
    "### 2. Bagging, Random Forests, Boosting\n",
    "\n",
    "* 2.1 Bagging\n",
    "* 2.2 Random Forests\n",
    "* 2.3 Boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2fOL1IBhB-o"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jc0SbuFeQBwW"
   },
   "source": [
    "### Load libraries which will be needed in this Notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1610125350172,
     "user": {
      "displayName": "Stephanie Valleau",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPxYC_xbIb6qmkXo8iMld6AvpYL5dO54osIaSx=s64",
      "userId": "13099634571785749992"
     },
     "user_tz": 480
    },
    "id": "XA8E1GTQQBwW",
    "outputId": "a0e03050-f4d1-4ad4-893c-079b81cd9097"
   },
   "outputs": [],
   "source": [
    "# Pandas library for the pandas dataframes\n",
    "import pandas as pd    \n",
    "import numpy as np\n",
    "\n",
    "# Import Scikit-Learn library for decision tree models\n",
    "import sklearn         \n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.utils import resample\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score\n",
    "\n",
    "\n",
    "# Import plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Set larger fontsize for all plots\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Command to automatically reload modules before executing cells\n",
    "# not needed here but might be if you are writing your own library \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cm6isl1eQBwX"
   },
   "source": [
    "## 1. What are decision tree methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description taken from [wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning): \n",
    "\n",
    "Decision tree learning is one of the predictive modelling approaches used in statistics, data mining and machine learning. It uses a **decision tree** (as a predictive model) to go from **observations** about an item (represented in the **_branches_**) to **conclusions** about the item's target value (represented in the **_leaves_**). \n",
    "\n",
    "Tree models where the target variable can take a _discrete set of values_ are called **classification trees**; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. \n",
    "\n",
    "Decision trees where the target variable can take continuous values (typically real numbers) are called **regression trees**. \n",
    "\n",
    "\n",
    "We can see the structure and terms to describe a tree below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://wiki.atlan.com/content/images/2019/10/Decision-tree-structure.png\" width='350' align=left>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an example of classification decision tree \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2017/07/Decision-Trees-Example.png\" width='300' align=left>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each node of the tree we are making a decision to ... eventually reach our target, a class or a number.\n",
    "\n",
    "**Some questions ...**\n",
    "\n",
    "* How do we build a tree?\n",
    "* How do we find the best tree?\n",
    "* Where and when should we create nodes / splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Regression decision trees\n",
    "\n",
    "Our aim is to find cutpoints or splits which minize the residual sum of squares RSS\n",
    "\n",
    "$$\\text{RSS} = \\sum_{i=1}^{N} \\left(y_i - \\hat{y}_i\\right)^2$$\n",
    "\n",
    "where $\\hat{y}_i$ is the estimated target regression value from the tree.\n",
    "\n",
    "How can we do that?\n",
    "\n",
    "**Top-down greedy** a.k.a. **recursive binary splitting** approach:\n",
    "\n",
    "**Training**\n",
    "\n",
    "You have some input data $X=\\left\\{X_1,X_2,...,X_p\\right\\}$ and a corresponding target $y$\n",
    "* _**step 1:**_ Divide your input data into a training and testing set\n",
    "* _**step 2:**_ Pick an input variable or predictor $X_j$ and a cutpoint $s$ which define two groups $R_1(j,s)=\\left\\{X|X_j<s\\right\\}$ and $R_2(j,s)=\\left\\{X|X_j\\geq s\\right\\}$ \n",
    "* _**step 3:**_ Repeat for each $j$ and cutpoint $s$ and find the values of $j$ and $s$ which minimize the weigthed average of the MSE's:\n",
    "\n",
    "$$\\sum_{i:x_i\\in R_1(j,s)}\\left(y_i-\\hat{y}^{{\\sf ave} }_{R_1}\\right)^2 + \\sum_{i:x_i\\in R_2(j,s)}\\left(y_i-\\hat{y}^{\\sf ave}_{R_2}\\right)^2 $$\n",
    "\n",
    "Here $\\hat{y}^{{\\sf ave} }_{R_1}$ is the average or mean response for the training set in $R_1$ .. same for the second one which is the mean in $R_2$\n",
    "\n",
    "**Testing**\n",
    "* **_step 4_**: Repeat steps 2-3 until you reach a stopping criterion or maximum depth or when you can no longer split (note this last option would result in overfitting). Note that at each iteration we are splitting from previous regions!\n",
    "\n",
    "Once your tree is built, you will **predict** the numerical response value $y_k^{\\sf{test}}$ by using the **mean of the training observations in the region $R_l$ where the test observation is found to belong** based on the tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression decision tree using `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datasets.load_boston()\n",
    "X, y = datasets.load_boston(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = DecisionTreeRegressor()#max_depth=3)\n",
    "regr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediction accuracy MSE: \", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does our decision tree look like? We will use the `tree.plot_tree` function. A nice blog on plotting decision trees can be found [here](https://mljar.com/blog/visualize-decision-tree/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(35,20))\n",
    "_ = tree.plot_tree(regr, feature_names=df.feature_names, filled=True, fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(y_test,y_pred,'*', color='royalblue', label=\"predicted\")\n",
    "plt.plot(y_pred,y_pred,'-', color='red', label=\"ideal\")\n",
    "plt.xlabel('y test set')\n",
    "plt.ylabel('y predicted')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.get_n_leaves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (5 min - Breakout room): Play with the `max_depth` parameter\n",
    "\n",
    "* How does the prediction change with `max_depth`? Try running the above for a set of growing values of `max_depth`\n",
    "\n",
    "* How does the tree change if you keep `max_depth=5` and change the training dataset (e.g. change the random seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did effectively can be seen in this picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1528907338/regression-tree_g8zxq5.png\" width='500' align=left>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problems**\n",
    "\n",
    "* While predictions on the training set are good, it usually overfits teh data - **high variance**\n",
    "* We might miss out on the best tree - because we are choosing in order (similarly to forward stepwise selection - multiparameter regression) \n",
    "* One solution is **pruning** (for more information see section 8.1 of textbook or [here](https://en.wikipedia.org/wiki/Decision_tree_pruning)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Classification decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can follow an approach which is almost identical to that used for regression decision trees. \n",
    "\n",
    "The main difference is that the target is a categorical variable - a class. Also RSS can't be used as an error rate. Instead we have a classification error rate just as in KNN and related methods, e.g. the Gini index or Entropy. For more details see - ISL 8.1.2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary on decision trees\n",
    "\n",
    "* Decision trees are easy to explain and interpret\n",
    "* Decision trees can be displayed graphically\n",
    "* Can be used both for regression and classification\n",
    "* Decision trees are not as accurate in terms of level of prediction accuracy respect to other regression and classification approaches\n",
    "* Decision trees are very sensitive to the data - small changes in the data lead to a large change in the final estimated tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bagging, Random Forests, Boosting\n",
    "\n",
    "We can reduce the problem of high variance for Decision Trees by using ensemble methods.\n",
    "\n",
    "### 2.1 Bagging\n",
    "\n",
    "Recall that the mean of $N$ observations each with variance $\\sigma^2$ is $\\frac{\\sigma^2}{N}$ - the more we sample, the more accurate our estimated mean. \n",
    "\n",
    "Based on this fact in **Bagging** the algorithm is:\n",
    "\n",
    "* **_Step 1_**: Create $B=\\left\\{B_1, B_2, ..., B_{N}\\right\\}$, i.e. $N$ separate training sets using Bootstrapping. Boostrapping saves us the problem of needing $N$ input training sets.\n",
    "* **_Step 2_**: Train on each $B_k \\in B$ to create an estimator $\\hat{f}^{\\,k}(x)$\n",
    "    * Consider all input feature pairs.\n",
    "    * Find the best split point from **all** input features by minimizing the weighted average of the MSE's (or e.g. Gini index if Classifying)\n",
    "    * Repeat until you reach maximum depth or a stopping criterion\n",
    "* **_Step 3_**: Average over the estimators to create a single low-variance estimator\n",
    "$$\\hat{f}_{{\\sf avg}}(x)=\\frac{1}{N}\\sum_{k=1}^{N}\\hat{f}^{\\,k}(x)$$\n",
    "\n",
    "\n",
    "**Advantage of Bagging**\n",
    "\n",
    "* While each bootstrap set trained estimator has a high variance and low bias the average over ~ hundreds, thousands largely _reduces the variance_\n",
    "\n",
    "* Note: Bagging can also be used to reduce the variance of other regression algorithms / can also be used for classification\n",
    "\n",
    "**Watch out** - with bagging we allow the algorithm to explore all $X$ predictors, this means that most initial splits will include the predictor which is strongest and minimizes the MSE - hence our bagged trees will be similar and **correlated**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at a decision tree for classification. We will load the Iris dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris = datasets.load_iris()\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=191)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `BaggingClassifier()` from sklearn - for more info see [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = DecisionTreeClassifier(max_depth=10)\n",
    "clf_bag = BaggingClassifier(base_estimator=estimator, n_estimators=50, random_state=42)\n",
    "\n",
    "clf_bag = clf_bag.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Estimator\", clf_bag.base_estimator_)\n",
    "print(\"Boostrapping with replacement\", clf_bag.bootstrap)\n",
    "print(\"Names of classes\", clf_bag.classes_)\n",
    "print(\"Number of classes\", clf_bag.n_classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_BSTR = clf_bag.predict(X_test)\n",
    "\n",
    "print(\"Mean accuracy on test set\", clf_bag.score(X_test, y_test))\n",
    "print(\"Accuracy score\", accuracy_score(y_test, y_BSTR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=y_BSTR,\n",
    "            s=60, cmap='viridis', edgecolor='k', alpha = 0.5, label='Bootstrap predicted')\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='+', c=y_test,\n",
    "            s=90, cmap='viridis', edgecolor='k', label='test set')\n",
    "plt.xlabel(df_iris.feature_names[0])\n",
    "plt.ylabel(df_iris.feature_names[1])\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random Forests\n",
    "\n",
    "The idea of random forests is to **decorrelate** trees by randomly selecting input features as split candidates \n",
    "\n",
    "* **_Step 1_**: Create $B=\\left\\{B_1, B_2, ..., B_N\\right\\}$, i.e. $N$ separate training sets using Bootstrapping. Boostrapping saves us the problem of needing $N$ input training sets.\n",
    "* **_Step 2_**: Train on each $B_k \\in B$ to create an estimator $\\hat{f}^{\\,k}(x)$ \n",
    "    * **Randomly select $m$ features from total $p$ input features** with  $p<< m$ - usually $m\\approx\\sqrt p$\n",
    "    * Find the best split point from the $m$ features by minimizing the weighted average of the MSE's (or e.g. Gini index if Classifying)\n",
    "    * Repeat for the next split point until you reach maximum depth or a stopping criterion\n",
    "* **_Step 3_**: Average over the estimators to create a single low-variance estimator\n",
    "$$\\hat{f}_{{\\sf avg}}(x)=\\frac{1}{N}\\sum_{k=1}^{N}\\hat{f}^{\\,k}(x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Use the Random forest approach on the Iris dataset (10 min)\n",
    "\n",
    "* Have a look at the sklearn `RandomForestClassifier()` [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "* See how the test set score changes as you increase the number of trees\n",
    "* Make sure that you are bootstrapping!\n",
    "* Plot your predicted classes using a scatter plot for `n_estimators = 50`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "clf_RF = clf_RF.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Estimator\", clf_RF.base_estimator_)\n",
    "print(\"Maximum depth\", clf_RF.max_depth)\n",
    "print(\"Boostrapping with replacement\", clf_RF.bootstrap)\n",
    "print(\"Names of classes\", clf_RF.classes_)\n",
    "print(\"Number of classes\", clf_RF.n_classes_)\n",
    "\n",
    "print(\"Mean accuracy on test set\", clf_RF.score(X_test, y_test))\n",
    "\n",
    "y_RF = clf_RF.predict(X_test)\n",
    "\n",
    "print(\"Accuracy score\", accuracy_score(y_test, y_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Boosting\n",
    "\n",
    "The main idea of Boosting is to learn from the data slowly by fitting a decision tree to the **residuals** of the model rather than to the target output Y.\n",
    "\n",
    "Boosting works **sequentially** - each tree is grown from previously trained trees. Boosting does not use bootstrapping. \n",
    "\n",
    "**Algorithm for Boosting**\n",
    "\n",
    "* **_Step 1_**: Define an initial estimator function $\\hat{f}(x)=0$ and residuals $r_i=y_i$ for all data points $i$ in the training set\n",
    "* **_Step 2_**: Select a number of trees $N$, and for each iteration in $B$, $b=1,2,...,N$\n",
    "    * Fit a tree $\\hat{f}^{\\,b}$ with a _chosen number of splits $d$_ to the training data (we will have $d+1$ terminal nodes)\n",
    "    * Update the overall estimator $\\hat{f}(x)$ as\n",
    "    $$\\hat{f}(x)\\leftarrow \\hat{f}(x)+\\lambda\\hat{f}^{\\,b}(x)$$ \n",
    "    where $\\lambda$ is a _shrinkage parameter_ usually $\\approx$ 0.01 or 0.001\n",
    "    * Update the residuals\n",
    "    $$r_i\\leftarrow r_i-\\lambda\\hat{f}^{\\,b}(x)$$\n",
    "* **_Step 3_**: The boosted model is obtained as\n",
    "$$\\hat{f}(x)=\\sum_{b=1}^{N}\\lambda\\hat{f}^{\\,b}(x) $$\n",
    "\n",
    "Some disadvantages of Boosting models:\n",
    "\n",
    "* Boosting models are more sensitive to overfitting if the data is noisy.\n",
    "* It takes longer to train Boosting models given that trees are built sequentially.\n",
    "* More difficult to fit than Random Forests because we have three parameters - number of trees $N$, shrinkage parameters $\\lambda$ and $d$ number of splits per tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can use the `sklearn` gradient boosting classifier function - documentation can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_GBM = GradientBoostingClassifier(n_estimators=1000, max_depth=10, learning_rate=0.1, random_state=42)\n",
    "clf_GBM = clf_GBM.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Maximum depth\", clf_GBM.max_depth)\n",
    "print(\"Names of classes\", clf_GBM.classes_)\n",
    "print(\"Number of classes\", clf_GBM.n_classes_)\n",
    "\n",
    "print(\"Mean accuracy on test set\", clf_GBM.score(X_test, y_test))\n",
    "\n",
    "y_GBM = clf_GBM.predict(X_test)\n",
    "\n",
    "print(\"Accuracy score\", accuracy_score(y_test, y_GBM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=y_GBM,\n",
    "            s=60, cmap='viridis', edgecolor='k', alpha = 0.5, label='GBM predicted')\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='+', c=y_test,\n",
    "            s=90, cmap='viridis', edgecolor='k', label='test set')\n",
    "plt.xlabel(df_iris.feature_names[0])\n",
    "plt.ylabel(df_iris.feature_names[1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for today! In your homework you will get to look at using some of these methods for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In class exercise (10 min breakout rooms) ... what is the maximum number of leaves?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you get back in the main room select one person from your breakout room to give an answer in the chat :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the boston dataset as loaded below and see what the maximum depth of the tree is. \n",
    "Use the `DecisionTreeRegressor()` function and fit it to the training data ... what is the value of the maximum number of leaves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datasets.load_boston()\n",
    "X, y = datasets.load_boston(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Iris dataset as loaded below and see what the maximum depth of the tree is. \n",
    "Use the `DecisionTreeClassifier()` function and fit it to the training data ... what is the value of the what is the value of the maximum number of leaves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris = datasets.load_iris()\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=191)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C-HACK Tutorial 5: Regression and Error for instructors.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (tunnel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
