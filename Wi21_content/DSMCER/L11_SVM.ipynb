{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhMpsTQGPQIm"
   },
   "source": [
    "# Data Science Methods for Clean Energy Research \n",
    "## _Support Vector Machines_\n",
    "\n",
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXXn9xy52KPj"
   },
   "source": [
    "\n",
    "### 1. Maximal margin classifier\n",
    "\n",
    "### 2. Support vector classifier\n",
    "\n",
    "### 3. Support vector machine\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2fOL1IBhB-o"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jc0SbuFeQBwW"
   },
   "source": [
    "### Load libraries which will be needed in this Notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1610125350172,
     "user": {
      "displayName": "Stephanie Valleau",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPxYC_xbIb6qmkXo8iMld6AvpYL5dO54osIaSx=s64",
      "userId": "13099634571785749992"
     },
     "user_tz": 480
    },
    "id": "XA8E1GTQQBwW",
    "outputId": "a0e03050-f4d1-4ad4-893c-079b81cd9097"
   },
   "outputs": [],
   "source": [
    "# Pandas library for the pandas dataframes\n",
    "import pandas as pd    \n",
    "import numpy as np\n",
    "\n",
    "# Import Scikit-Learn library for decision tree models\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import sv_utils as sv\n",
    "\n",
    "# Import plotting libraries\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "# Set larger fontsize for all plots\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import cm\n",
    "\n",
    "# Command to automatically reload modules before executing cells\n",
    "# not needed here but might be if you are writing your own library \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cm6isl1eQBwX"
   },
   "source": [
    "## What are SVM methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM methods are used for classification. They are based on the idea of defining classes based on a separating **hyperplane**\n",
    "\n",
    "Hyperplanes in a $p$ dimensional space are defined as \n",
    "\n",
    "$$\\beta_0+\\beta_1X_1 + ...+\\beta_pX_p=0$$\n",
    "\n",
    "So in two dimensions the equation is \n",
    "\n",
    "$$\\beta_0 + \\beta_1X_1 +\\beta_2X_2 =0  $$\n",
    "\n",
    "which is simply the equation of a line in the $X_1,X_2$ space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.my_special_margin_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define whether a point lies in one class or an other depending on the sign of the equation - i.e. depending on which side of the plane it lies on:\n",
    "\n",
    "Blue class:\n",
    "$$\\beta_0+\\beta_1X_1 + ...+\\beta_pX_p>0$$\n",
    "or\n",
    "\n",
    "Orange class:\n",
    "$$\\beta_0+\\beta_1X_1 + ...+\\beta_pX_p<0$$\n",
    "\n",
    "In 3 dimensions, the hyperplane would be a two dimensional surface and so on.\n",
    "\n",
    "The hyperplane is our **decision boundary**.\n",
    "\n",
    "Now, how do we define and find this hyperplane?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Maximal Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/72/SVM_margin.png\" width='300' align=left>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the margin is equal to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to find the hyperplane:\n",
    "\n",
    "* Compute the perpendicular distance from each training obervation $x^*$ to a given separating hyperplane which is a multiple linear function\n",
    "$$f(x^*)=\\beta_0+\\beta_1x_1^*+...+\\beta_px_p^*$$\n",
    "    * Compute the distance as the dot product between $f(x^*)$ and $y^*$\n",
    " \n",
    "* Sort these distances - the smallest distance is the *margin*, M\n",
    "* Repeat for a new set of values of parameters $\\beta$ and choose our hyperplane (i.e. the values of $\\beta_i$) as the one that has the largest margin\n",
    "\n",
    "   * Note: We constrain $\\sum_{j=1}^{p}\\beta_j^2=1$ and $M > 0$\n",
    "   \n",
    "This is a so called **hard margin** as we assume that a separating hyperplane exists and that there is a solution with $M>0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some data to be classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1, class_sep=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and split it into a train and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what does this look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"mediumslateblue\",\"mediumturquoise\",\"coral\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], \n",
    "            c=y_train, cmap=my_cmap, alpha=0.8,\n",
    "           edgecolors='k', label='Training')\n",
    "\n",
    "# Plot the testing points\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='>', \n",
    "            c=y_test, cmap=my_cmap, alpha=0.8,\n",
    "           edgecolors='k', label='Test')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `sklearn` SVC [function](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) to find the hyperplane which divides this data into two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Score: \", score)\n",
    "print(\"Accuracy score:\", accuracy_score(y_test,clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the decision boundary in a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "\n",
    "h=0.02\n",
    "delt=0.5\n",
    "\n",
    "xx, yy = sv.make_meshgrid(X[:,0], X[:,1], h, delt)\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "ax.contourf(xx, yy, Z, cmap=my_cmap, alpha=0.3)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=my_cmap)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=my_cmap,\n",
    "           edgecolors='k', marker='o', label='Training')\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=my_cmap,\n",
    "           edgecolors='k', marker='>', label='Test')\n",
    "\n",
    "plt.xlabel('$X_1$')\n",
    "plt.ylabel('$X_2$')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, this dataset has two distinct groups, so separating the two with a line is pretty simple. However, that's not always the case. This method would fall apart if there was any overlap. \n",
    "\n",
    "What happens when there is **no separable boundary**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=2, class_sep=0.5)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.3, random_state=42)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], \n",
    "            c=y_train, cmap=my_cmap, alpha=0.8,\n",
    "           edgecolors='k', label='Training')\n",
    "\n",
    "# Plot the testing points\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='>', \n",
    "            c=y_test, cmap=my_cmap, alpha=0.8,\n",
    "           edgecolors='k', label='Test')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Support vector classifier\n",
    "\n",
    "The concept of the separable hard decision boundary is generalized to the non-separable case - we allow hyperplanes which **do not** perfectly separate the two (or more) classes - sometimes referred to as a soft margin. \n",
    "\n",
    "* Some points will be incorrectly classified but overall we will get a better classification of most of the training points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://miro.medium.com/max/552/1*CD08yESKvYgyM7pJhCnQeQ.png\" width='500' align=left>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we find the soft margin hyperplane?\n",
    "\n",
    "In the previous case our optimization problem was \n",
    "$$y_i\\cdot f(x_i)=y_i\\cdot(\\beta_0+\\beta_1x_{i,1}+...+\\beta_px_{i,p})\\geq M$$\n",
    "\n",
    "with $M> 0$  and $\\sum_j\\beta_j^2=1$ \n",
    "\n",
    "Now we soften the margin by introducing **slack variables** $\\epsilon_i$\n",
    "\n",
    "$$y_i\\cdot f(x_i)=y_i\\cdot(\\beta_0+\\beta_1x_{i,1}+...+\\beta_px_{i,p})\\geq M(1-\\epsilon_i$$\n",
    "\n",
    "with $\\epsilon_i\\geq0$ and $\\sum_{i=1}^{n}\\leq C$ with $C$ a non negative tuning parameter.\n",
    "\n",
    "Just as we did previously, we aim to find the large margin $M$. Note thtat now we have more variables to determine, beyond the $\\beta_i$ we have $\\epsilon_i$ parameters.\n",
    "\n",
    "The C parameter represents the “budget” for the amount of points that can violate the decision boundary during training as C increases, the extent of allowed violations increases. We can think of C as our bias/variance tradeoff parameter.\n",
    "\n",
    "* If C=0 we are in the case of the hard margin\n",
    "* If C is large we have a small margin\n",
    "* If C is small we have a large margin\n",
    "* Observations that lie directly on the margin or on the wrong side of the margin for their class are known as support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = [ 1000, 100, 10, 1, 0.1, 0.001]\n",
    "\n",
    "figure, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 9))\n",
    "\n",
    "axes = [item for sublist in axes for item in sublist]\n",
    "\n",
    "xx, yy = sv.make_meshgrid(X[:,0], X[:,1], h, delt)\n",
    "\n",
    "for C, ax in zip(C_values, axes):\n",
    "    \n",
    "    clf = SVC(kernel='linear', C=C)\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "\n",
    "    # Plot the decision boundary areas. \n",
    "    _, Z = sv.plot_decision_contours(ax, clf, xx, yy, cmap=my_cmap, alpha=0.3)\n",
    "    \n",
    "    # Plot decision boundary and margins\n",
    "    ax.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.7,\n",
    "           linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # Plot the training points\n",
    "    #ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "    #           edgecolors='k', label='Training')\n",
    "    # Plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=my_cmap,\n",
    "                edgecolors='k', marker='>', s=70, alpha=0.7, label='Test')\n",
    "\n",
    "    # ax.legend(loc=\"upper center\")\n",
    "    ax.set_title('C Value: '+str(1./C)+'\\nScore: '+str(round(score,2)))\n",
    "\n",
    "    ax.set_xlabel('$X_1$')\n",
    "    ax.set_ylabel('$X_2$')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Support Vector Machines\n",
    "\n",
    "Can we do anything better than a simple linear boundary? With KNN we were able to capture non linear decision boundaries .. \n",
    "\n",
    "We can do the same thing with Support Vector Machines!\n",
    "\n",
    "SVMs really get their power when you take advantage of something called a kernel. A kernel takes your original dataset, transforms it into higher dimensional space, and then draws a hyperplane in that new hyperdimensional space. Such hyperplane is linear in the hyperdimensional space but it **becomes non linear** in the original predictor space. It's a lot of math that we won't go into here. But let's see what happens when we try to classify some data with a radial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Create data arranged in a circular pattern\n",
    "data = make_circles(noise=0.2, factor=0.5, random_state=1)\n",
    "X, y = data\n",
    "\n",
    "# Split in train and test set\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "# Fit an SVC classifier using a linear kernel\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "xx, yy = sv.make_meshgrid(X[:,0], X[:,1], h, delt)\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(4,4))\n",
    "sv.plot_decision_contours(ax, clf, xx, yy, cmap=my_cmap, alpha=0.3)\n",
    "\n",
    "# Plot the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=my_cmap,\n",
    "           edgecolors='k', marker='o', label='Training')\n",
    "# Plot the testing points\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=my_cmap,\n",
    "           edgecolors='k', marker='>', alpha=0.6, label='Test')\n",
    "ax.set_xlabel('$X_1$')\n",
    "ax.set_ylabel('$X_2$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define a classifier object using the radial kernel\n",
    "clf = SVC(kernel='rbf', gamma=4, C=0.1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = sv.make_meshgrid(X[:,0], X[:,1], h, delt)\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(4,4))\n",
    "\n",
    "# Plot the decision boundary.\n",
    "sv.plot_decision_contours(ax, clf, xx, yy, cmap=my_cmap, alpha=0.3)\n",
    "\n",
    "# Plot the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=my_cmap,\n",
    "           edgecolors='k', marker='o', label='Training')\n",
    "# Plot the testing points\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=my_cmap,\n",
    "           edgecolors='k', marker='>',alpha=0.8, label='Test')\n",
    "\n",
    "ax.set_xlabel('$X_1$')\n",
    "ax.set_ylabel('$X_2$')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are additional kernels that you can try. If you want to build your SVM skills, try some of these additional exercises:\n",
    "\n",
    "* Calculate the MSE on the above training and test datasets and compare each of the models.\n",
    "* Look up the types of kernels available, and try using some alternate kernels.\n",
    "* Compare the KNN classifier to the linear and radial SVMs\n",
    "\n",
    "\n",
    "Note - although SVM's are mostly used for two class problems they can be used when you have more than two classes - for more info see [here](https://scikit-learn.org/stable/modules/svm.html)\n",
    "\n",
    "## Breakout room: SVM for the Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's carry out SVM for the Iris dataset (based on the sklearn [tutorial](https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html))\n",
    "* Import the Iris data set and split it into a train and test set. Only keep the first two input features in X.\n",
    "* Define three classifiers, `SCV(kernel=...,**kwargs)`: SVM with a linear, radial and third degree polynomial kernel `'linear', 'rbf', 'poly'`\n",
    "* Fit these to the training data (set `C=1`) and predict on the test set \n",
    "\n",
    "* Plot the decision boundaries in each case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C-HACK Tutorial 5: Regression and Error for instructors.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (tunnel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
