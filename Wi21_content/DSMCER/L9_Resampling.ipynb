{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhMpsTQGPQIm"
   },
   "source": [
    "# Data Science Methods for Clean Energy Research \n",
    "## _Resampling_\n",
    "\n",
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXXn9xy52KPj"
   },
   "source": [
    "\n",
    "### 1. Cross-validation\n",
    "\n",
    "* 1.1 Validation set\n",
    "* 1.2 Leave-One-Out cross validation\n",
    "* 1.3 k-Fold cross validation\n",
    "\n",
    "### 2. Bootstrapping\n",
    "\n",
    "* 2.1 Finding the variance on the parameters of a regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2fOL1IBhB-o"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jc0SbuFeQBwW"
   },
   "source": [
    "### Load libraries which will be needed in this Notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1610125350172,
     "user": {
      "displayName": "Stephanie Valleau",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPxYC_xbIb6qmkXo8iMld6AvpYL5dO54osIaSx=s64",
      "userId": "13099634571785749992"
     },
     "user_tz": 480
    },
    "id": "XA8E1GTQQBwW",
    "outputId": "a0e03050-f4d1-4ad4-893c-079b81cd9097"
   },
   "outputs": [],
   "source": [
    "# Pandas library for the pandas dataframes\n",
    "import pandas as pd    \n",
    "\n",
    "# Import Scikit-Learn library for the regression models\n",
    "import sklearn         \n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Import numpy \n",
    "import numpy as np\n",
    "\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "\n",
    "# Import plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "# Set larger fontsize for all plots\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Command to automatically reload modules before executing cells\n",
    "# not needed here but might be if you are writing your own library \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cm6isl1eQBwX"
   },
   "source": [
    "## 1. Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised ML methods we evaluate the validity of our estimates by using the test set. Suppose that you only have a very small test set, or no test set ... how can you estimate the test error? \n",
    "\n",
    "One way is to use Cross-Validation by _holding out_ a subset of the training samples and updating the set which is held out at each iteration of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Validation set \n",
    "\n",
    "We have seen this already ... the idea of the Validation set approach is to split our data into a training and test set a.k.a _validation_ set and to look at the MSE of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = datasets.load_diabetes()\n",
    "\n",
    "X, y = dset.data, dset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:,0], y_train, s=60, marker='h', c='paleturquoise', edgecolors='teal', label='training set')\n",
    "plt.scatter(X_test[:,0], y_test, s=60, marker='P', c='mistyrose', edgecolors='darkviolet',label='test set')\n",
    "plt.ylabel(\"target\")\n",
    "plt.xlabel(dset.feature_names[0])\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.65, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression().fit(X_train, y_train)\n",
    "y_predict = reg.predict(X_test)\n",
    "r2 = r2_score(y_test, y_predict)\n",
    "MSE = mean_squared_error(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"r2 score:\", r2, \" MSE:\", MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In class exercise - breakout room (15 min)\n",
    "\n",
    "* Work on writing a function to repeat what we did for different test sets - you can change the seed or the size of the test set\n",
    "\n",
    "* Run your function a few times and save the error metric ($R^2$ or MSE)\n",
    "\n",
    "* Plot your $R^2$ or MSE for all iterations - what do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(X, y, seed, split):\n",
    "    #....  \n",
    "    return r2, MSE, y_predict, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_list = []\n",
    "MSE_list = []\n",
    "N_test_samples = 100\n",
    "\n",
    "plt.figure()\n",
    "for i in range(N_test_samples):\n",
    "    # ....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems with this approach?\n",
    "\n",
    "* **1)** **Depends on** the choice of points in the **test set** and in the **training set** - as you can see even by keeping the number of points constant we have a different MSE and $R^2$ score for each test set!\n",
    "\n",
    "* **2)** Given that it has never seen the test set but has seen the training set by estimating the error only on the test set we might **overestimate the error** on the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Cross-validation: _Leave-One-Out Cross-Validation_\n",
    "\n",
    "Leave-One-Out Cross Validation LOOCV process:\n",
    "\n",
    "* **_step 1_** Divide dataset containing $N$ points in two parts: one part contains a single data point $(x_1,y_1)$ the other contains the rest of the $N-1$ points\n",
    "* **_step 2_** Train the model on the $N-1$ point set\n",
    "* **_step 3_** Compute the MSE for the $(x_1,y_1)$ hold-out point\n",
    "* Repeat steps 1-3 by selecting the next point $i$ in the set of $N$ points as the hold-out point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://miro.medium.com/max/2816/1*AVVhcmOs7WCBnpNhqi-L6g.png\" width='400' align=left>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then estimate the test MSE by averaging over the $N$ test errors:\n",
    "\n",
    "$$\\text {MSE}^{ave}_{\\text{test}}\\equiv\\text{CV}_{(N)}=\\frac{1}{N}\\sum_{i=1}^N \\text{MSE}_i$$\n",
    "\n",
    "\n",
    "\n",
    "One disadvantage is that we need to make $N$ evaluations, if $N$ is large that can be time demanding.\n",
    "\n",
    "Let's implement this for the diabetese dataset using the `LeaveOneOut()` [function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html) implemented in scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_loo = []\n",
    "for train_index, test_index in loo.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    #print(\"X train\", X_train, \"\\nX test\",  X_test, \"\\ny train\", y_train, \"\\ny test\", y_test)\n",
    "    reg = linear_model.LinearRegression().fit(X_train, y_train)\n",
    "    y_predict = reg.predict(X_test)\n",
    "    MSE_loo.append(mean_squared_error(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(MSE_loo,'x',color='crimson')\n",
    "plt.xlabel('Test set')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_mean_MSE = 1/len(y) * np.sum(MSE_loo)\n",
    "\n",
    "print(\"Mean test MSE using LOOCV = \", CV_mean_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions on LOOCV?\n",
    "\n",
    "* Will the results change if we rerun the method?\n",
    "* Which of the Validation problems are we helping to solve in LOOCV?\n",
    "* Is holding out a single point each time the best option?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 _k_-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_k_-Fold Cross-Validation process\n",
    "\n",
    "* **_step 1_** Divide dataset containing $N$ points in $k$ groups or **_folds_** - each fold contains $\\approx N/k$ \n",
    "* **_step 2_** Keep the first fold as a validation set and train the model on the subsequent $k-1$ folds\n",
    "* **_step 3_** Compute the MSE for the first fold\n",
    "* Repeat steps 1-3 by selecting the next fold $i$ in the set of $k$ folds as the validation set\n",
    "\n",
    "Let's try it out using the `KFold()` [function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_kf = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    #print(\"X train\", X_train, \"\\nX test\",  X_test, \"\\ny train\", y_train, \"\\ny test\", y_test)\n",
    "    reg = linear_model.LinearRegression().fit(X_train, y_train)\n",
    "    y_predict = reg.predict(X_test)\n",
    "    MSE_kf.append(mean_squared_error(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(MSE_kf,'*',color='blue')\n",
    "plt.xlabel('Test set k-fold')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of k-fold Cross-Validation\n",
    "\n",
    "* **Cheaper than LOOCV** as we only need $k$ iterations instead of $N$ (LOOCV is basically k-fold CV with $k=N$)\n",
    "* Variability in MSE is smaller than what we had with the Validation set approach (some of the training data is seen a few times)\n",
    "* Often gives **better estimates of the test error** rate than LOOCV due to the bias-variance tradeoff.\n",
    "    * LOOCV has lower bias than k-fold CV when $k< N$ (sees more of the population)\n",
    "    * LOOCV has higher variance than k-fold CV when $k<N$ ( less overlap between training sets)\n",
    "    * From the bias-variance tradeoff, values of $k$ which lead to models which does not suffer from high bias of variance are $k=5$ and $k=10$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example use of cross validation is to train deep neural networks DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    \n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width='400' align=left>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bootstrapping\n",
    "\n",
    "Bootstrapping refers to tests or metrics that uses **random sampling with replacement**. It is part of the class of **resampling** methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates.\n",
    "\n",
    "Bootstrapping is very useful to estimate the accuracy of an estimator .. for instance the variance on linear regression coefficients\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "* Randomly draw $n$ data points, with replacement, from your training data set (size = $n$).\n",
    "* Train your model and calculate coefficients/parameters\n",
    "* Repeat sampling many times (usually 1000), and calculate the variance of your coefficients\n",
    "\n",
    "\n",
    "### 2.1 Exercise: Bootstrapping for the Cheetah dataset (10 min)\n",
    "\n",
    "Let's use this for our cheetah dataset! We want to estimate the error on our estimate of each coefficient when using multiple linear regression.\n",
    "\n",
    "We will use the scikit-learn `resample()` function - for more info see [here](https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html)\n",
    "\n",
    "To solve this problem:\n",
    "* Use the `resample()` function to extract 20 samples from X and y (we loaded these from the diabetese dataset a few cells back) with `replace=True` - Note: each sample should be different! \n",
    "* Fit a multiple linear regression to each model and store the coefficients and intercept\n",
    "* Evaluate the mean and standard deviation of the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheetah_df=pd.read_csv('running_cheetah.csv',index_col=0)\n",
    "cheetah_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cheetah_df[['time [hr]','speed [miles/hr]']].values.reshape(-1, 2)\n",
    "y = cheetah_df['position [miles]'].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_samp = 5\n",
    "coeffs = []\n",
    "intercepts = []\n",
    "\n",
    "for i in range(N_samp):\n",
    "    # ....\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see how things change as you change the number of samples you bootstrap ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C-HACK Tutorial 5: Regression and Error for instructors.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (tunnel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
