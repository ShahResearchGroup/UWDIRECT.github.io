{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhMpsTQGPQIm"
   },
   "source": [
    "# Data Science Methods for Clean Energy Research \n",
    "## *Linear regression*\n",
    "\n",
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXXn9xy52KPj"
   },
   "source": [
    "In this tutorial we will look at fitting data to a curve using **regression** - our target output is therefore numerical. Specifically we will look at using **linear regression** to make **predictions** for new data points by dividing our data into a training and a testing set. Finally we will examine how much error we make in our fit and then in our predictions by computing the mean squared error, at the variance and bias.\n",
    "\n",
    "### 1. Introduction to linear regression\n",
    "\n",
    "* Simple linear regression with least mean squares\n",
    "\n",
    "### 2. Example of linear regression \n",
    "\n",
    "* Using linear regression to fit the position of a Cheetah\n",
    "\n",
    "### 3. Accuracy of estimated coefficients\n",
    "\n",
    "* Standard error and regression score \n",
    "\n",
    "### 4. Bias and variance\n",
    "\n",
    "* Estimating bias and variance on the Cheetah dataset\n",
    "* Unbiased estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2fOL1IBhB-o"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jc0SbuFeQBwW"
   },
   "source": [
    "### Load libraries which will be needed in this Notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1610125350172,
     "user": {
      "displayName": "Stephanie Valleau",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPxYC_xbIb6qmkXo8iMld6AvpYL5dO54osIaSx=s64",
      "userId": "13099634571785749992"
     },
     "user_tz": 480
    },
    "id": "XA8E1GTQQBwW",
    "outputId": "a0e03050-f4d1-4ad4-893c-079b81cd9097"
   },
   "outputs": [],
   "source": [
    "# Pandas library for the pandas dataframes\n",
    "import pandas as pd    \n",
    "\n",
    "# Import Scikit-Learn library for the regression models\n",
    "import sklearn         \n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Import numpy \n",
    "import numpy as np\n",
    "\n",
    "# Import plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Set larger fontsize for all plots\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Command to automatically reload modules before executing cells\n",
    "# not needed here but might be if you are writing your own library \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cm6isl1eQBwX"
   },
   "source": [
    "## 1. What is linear regression? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crBj3b4FHemQ"
   },
   "source": [
    "It is the process of predicting a **_dependent_** variable _Y_ based on the **_independent_** variable _X_ \n",
    "\n",
    "$$Y = f(X) + \\epsilon\\;\\;\\;\\;\\;\\sf{eq. 1}$$ \n",
    "using a linear function\n",
    "$$Y \\approx \\beta_0 + \\beta_1 X\\;\\;\\;\\;\\;\\sf{eq. 2}$$\n",
    "\n",
    "Here $\\beta_0$ is the intercept and $\\beta_1$ the slope, toghether we refer to these are **_coefficients_** or **_parameters_**. Given that the linear relationship is an assumption and that the coefficients are unknown, once we have trained the model, we will have created an estimator, which can estimate the value of $Y$ for a given observed value $x$:\n",
    "\n",
    "$$\\hat{y} = \\hat{\\beta}_0+\\hat{\\beta}_1x\\;\\;\\;\\;\\;\\sf{eq. 3}$$\n",
    "\n",
    "\n",
    "Now, how do we find the optimal values of the coefficients? \n",
    "\n",
    "\n",
    "One way is by minimizing the **residual sum of squares** (RSS). Given a sample of $N$ points $\\left\\{(x_0,y_0),...,(x_N,y_N)\\right\\}$, we can write\n",
    "\n",
    "$${\\sf RSS} =\\sum_{i=1}^{N}\\left(y_i - \\hat{y}(x_i)\\right)^2 = \\sum_{i=1}^{N}\\left(y_i - \\hat{\\beta}_0 -\\hat{\\beta}_1 x_i  \\right)^2\\;\\;\\;\\;\\sf{eq. 4}$$\n",
    "\n",
    "One can show that by taking the derivatives of RSS respect to $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ and setting these to zero, the optimal values are\n",
    "\n",
    "$$\\hat{\\beta}_1=\\frac{\\sum_{i=1}^N(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^N(x_i-\\bar{x})^2}\\;\\;\\;\\;\\;\\sf{eq. 5}$$\n",
    "\n",
    "$$\\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\bar{x}\\;\\;\\;\\;\\;\\sf{eq. 6}$$\n",
    "\n",
    "with $\\bar{y}=\\sum_{i=1}^N\\frac{y_i}{N}$ and  $\\bar{x}=\\sum_{i=1}^N\\frac{x_i}{N}$ the sample means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this out in python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qppNLSFvQBwX"
   },
   "source": [
    "## 2.  Linear regression + a Cheetah: fitting with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOjPYw3SQBwX"
   },
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAE_OOndQBwX"
   },
   "source": [
    "Consider a Cheetah on the run (worth watching): https://www.youtube.com/watch?v=8-9oFxYFODE\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFPA80XH1qeu"
   },
   "source": [
    "![Image of Cheetah](https://drive.google.com/uc?export=view&id=1lpZinFnWbbreZxx2BxpldWPVBlAyUL89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0r6rBAhQBwX"
   },
   "source": [
    "Assume you have a set of data with values (time, position, speed, energy) for one Cheetah's motion. The dataset is in csv format, lets load it by using the <code> Pandas </code> library - the name of the file is <code> running_cheetah.csv </code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llseELesQBwY"
   },
   "outputs": [],
   "source": [
    "# Load the dataset using the read_csv() pandas function - we have to indicate that\n",
    "# the index of each row is in the first column\n",
    "cheetah_df=pd.read_csv('running_cheetah.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAcHDd1vQBwY"
   },
   "source": [
    "\n",
    "What does the data look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 1163,
     "status": "ok",
     "timestamp": 1610125487986,
     "user": {
      "displayName": "Stephanie Valleau",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPxYC_xbIb6qmkXo8iMld6AvpYL5dO54osIaSx=s64",
      "userId": "13099634571785749992"
     },
     "user_tz": 480
    },
    "id": "0-TMjSfuQBwY",
    "outputId": "7c34c23c-82a8-49bc-eb09-edce53b12966"
   },
   "outputs": [],
   "source": [
    "cheetah_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vavdktFQBwZ"
   },
   "source": [
    "### Let's visualize the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy3XoXStQBwZ"
   },
   "source": [
    "We can create a scatter plot of positions of the Cheetah as a function of time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "executionInfo": {
     "elapsed": 876,
     "status": "ok",
     "timestamp": 1610124028600,
     "user": {
      "displayName": "Stephanie Valleau",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPxYC_xbIb6qmkXo8iMld6AvpYL5dO54osIaSx=s64",
      "userId": "13099634571785749992"
     },
     "user_tz": 480
    },
    "id": "36JsJFWNQBwZ",
    "outputId": "6289e398-f6fe-4f49-c6ce-1565eb2c89c2"
   },
   "outputs": [],
   "source": [
    "# Using the pandas plot.scatter\n",
    "cheetah_df.plot.scatter('time [hr]', 'position [miles]', s=100, marker='.', color=\"cornflowerblue\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gc60p76jQBwZ"
   },
   "source": [
    "We will use [scikit-learn](https://scikit-learn.org/stable/) as for KNN, it contains a function called [linear regression](https://scikit-learn.org/stable/modules/linear_model.html). \n",
    "\n",
    "The function we will use is called <code> LinearRegression() </code>. \n",
    "\n",
    "Let's start by creating a **training** and **testing** data set from the orginal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cheetah_df['time [hr]'].values.reshape(-1, 1)\n",
    "y = cheetah_df['position [miles]'].values.reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxPnpy8jQBwZ"
   },
   "source": [
    "### Fitting the Cheetah position in time data to a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1610124033838,
     "user": {
      "displayName": "Stephanie Valleau",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPxYC_xbIb6qmkXo8iMld6AvpYL5dO54osIaSx=s64",
      "userId": "13099634571785749992"
     },
     "user_tz": 480
    },
    "id": "AUWdaOgMQBwZ",
    "outputId": "09e4c5d2-eb6b-49ad-9bec-6ff21988559f"
   },
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model to fit to the training data, the X values are times and the Y values are positions of the Cheetah\n",
    "regr.fit(X_train, y_train)\n",
    "beta1 = regr.coef_[0][0]\n",
    "beta0 = regr.intercept_[0]\n",
    "\n",
    "# Print the slope m and intercept b\n",
    "print('Scikit learn - Slope: ', beta1 , 'Intercept: ', beta0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uFUK6QfQBwa"
   },
   "source": [
    "Let's **predict** the values of $y$ in the test set using our fitted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DFlw26PQBwa"
   },
   "outputs": [],
   "source": [
    "# From the equation\n",
    "Y_calc_test_2 = beta1*X_test + beta0\n",
    "\n",
    "# Another way to get this is using the regr.predict function\n",
    "Y_calc_test = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVoA-OG4QBwa"
   },
   "source": [
    "Let's create a graphical visualization of the fitted values respect to the exact values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 592,
     "status": "ok",
     "timestamp": 1610124040664,
     "user": {
      "displayName": "Stephanie Valleau",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPxYC_xbIb6qmkXo8iMld6AvpYL5dO54osIaSx=s64",
      "userId": "13099634571785749992"
     },
     "user_tz": 480
    },
    "id": "cI6s967TQBwa",
    "outputId": "bf46ca50-04eb-49dd-a631-8197efd4fb1d"
   },
   "outputs": [],
   "source": [
    "# Lets plot exact positions respect to the time values using a scatter plot\n",
    "plt.scatter(X_train, y_train, s=50, marker='o', color=\"cornflowerblue\", alpha=0.8, label=\"training set\")\n",
    "plt.scatter(X_test, y_test, s=100, marker='>', color=\"mediumorchid\", alpha=0.8, label=\"test set\")\n",
    "plt.xlabel('time [hours]')\n",
    "plt.ylabel('position [miles]')\n",
    "\n",
    "# Now we compare to our fit by plotting both Y_cal and Y_calc_2 respect to time \n",
    "plt.plot(X_test, Y_calc_test, color='crimson',linewidth=3, label='linear fit')\n",
    "plt.plot(X_test, Y_calc_test_2,':', color='black',linewidth=2, label='linear fit check')\n",
    "plt.legend(bbox_to_anchor=(1.8, 1), loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSsBFNvcQBwb"
   },
   "source": [
    "## 3. How much error are we making? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKfamij9QBwb"
   },
   "source": [
    "### Accuracy of the coefficient estimates\n",
    "\n",
    "We can look at the standard error, which one can derive and find to be\n",
    "\n",
    "$$\\text{SE}(\\hat{\\beta}_0)^2=\\sigma^2\\left[\\frac{1}{N}+\\frac{\\bar{x}^2}{\\sum_{i=1}^N(x_i-\\bar{x})^2}\\right]\\;\\;\\;\\;\\sf{eq. 7}$$\n",
    "$$\\text{SE}(\\hat{\\beta}_1)^2=\\frac{\\sigma^2}{\\sum_{i=1}^N(x_i-\\bar{x})^2}\\;\\;\\;\\;\\;\\sf{eq. 8}$$\n",
    "\n",
    "and $\\sigma^2=\\text{Var}(\\epsilon)$. Here $\\sigma^2$ is unknown but usually estimated from the data and in this case defined as the residual standard error, RSE:\n",
    "\n",
    "$$\\sigma \\equiv \\text{RSE}= \\sqrt{\\frac{\\text{RSS}}{N-2}}\\;\\;\\;\\;\\;\\sf{eq. 9}$$\n",
    "\n",
    "with RSS the residual sum of squares, i.e.\n",
    "\n",
    "$$\\text{RSS} = \\sum_{i=1}^{N} \\left(y_i - \\hat{y}(x_i)\\right)^2\\;\\;\\;\\;\\;\\sf{eq. 10}$$\n",
    "\n",
    "we can normalize the RSS by the number of points N to retrieve the MSE as a metric of error of our fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1610124043659,
     "user": {
      "displayName": "Stephanie Valleau",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPxYC_xbIb6qmkXo8iMld6AvpYL5dO54osIaSx=s64",
      "userId": "13099634571785749992"
     },
     "user_tz": 480
    },
    "id": "BDCUEKNQQBwb",
    "outputId": "3a3fd2f5-e804-452c-85c6-6487079f7caf"
   },
   "outputs": [],
   "source": [
    "# The mean squared error\n",
    "print('Mean squared error: %.2f' % mean_squared_error(Y, Y_calc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5m88vMlkQBwb"
   },
   "source": [
    "Another way to measure error is the regression score, $R^2$. $R^2$ is generally defined as the ratio of the total sum of squares $\\text{TSS} $ to the residual sum of squares $\\text {RSS}$:\n",
    "\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}}\\;\\;\\;\\;\\sf{eq.11}$$\n",
    "\n",
    "with \n",
    "\n",
    "$$\\text{TSS}=\\sum_{i=1}^N \\left(y_i-\\bar{y}\\right)^2\\;\\;\\;\\;\\sf{eq.12}$$\n",
    "\n",
    "The best value of $R^2$ is 1 but it can also take a negative value if the error is large.\n",
    "\n",
    "See all the different regression metrics [here](https://scikit-learn.org/stable/modules/model_evaluation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 863,
     "status": "ok",
     "timestamp": 1610124047344,
     "user": {
      "displayName": "Stephanie Valleau",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPxYC_xbIb6qmkXo8iMld6AvpYL5dO54osIaSx=s64",
      "userId": "13099634571785749992"
     },
     "user_tz": 480
    },
    "id": "Ejt0nyHJQBwb",
    "outputId": "7e9cf90c-53f9-4a29-94f2-f7075547ff68"
   },
   "outputs": [],
   "source": [
    "# Print the coefficient of determination - 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, Y_calc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApcE4sHhQBwb"
   },
   "source": [
    "## 4. Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0fkdCeBQBwb"
   },
   "source": [
    "We could have used a part of the data to fit our line - **_training_** data - and the rest to validate it - **_test_** data. At that point, the <code> predict() </code> function would have predicted previoulsy unseen positions. In the case when a subset of training data is used to find the parameters of the model we refer to the model as *_trained_* once the optimal values of the parameters have been determined.\n",
    "\n",
    "Here, we are interested in evaluating the accuracy of the trained model when it predicts data from the test dataset. As we saw in lecture this morning, the error can be divided in to an **_irreducible_** and **_reducible_** part. \n",
    "\n",
    "Recall that the **_reducible error_** of the model , i.e. the error which comes from the mismatch of the model estimator function $\\hat{y}=\\hat{\\beta}_1x+\\hat{\\beta}_0$ respect to the exact data can be broken down into **_bias_** and **_variance_**. Let's see what these mean when looking at the problem of the Cheetah. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qs2K_vbzQBwb"
   },
   "source": [
    "### 4.1  Variance\n",
    "\n",
    "Let's go back to our Cheetah problem and train the model on three different sets of training points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ib5yfvvTQBwb"
   },
   "outputs": [],
   "source": [
    "# Define three datasets from the original set by selecting a range of values using the np.argwhere() function\n",
    "\n",
    "# Search for positions in each numpy array where the values are in a specific range of time e.g. time < 35 hr\n",
    "indices_test = [ind[0] for ind in np.argwhere(X <= 35)]\n",
    "indices1 = [ind[0] for ind in np.argwhere((X > 35) & (X < 50))]\n",
    "indices2 = [ind[0] for ind in np.argwhere((X >= 50) & (X < 70))]\n",
    "indices3 = [ind[0] for ind in np.argwhere(X >= 70)]\n",
    "\n",
    "# Define the three training dataset and testing datasets by using the indices \n",
    "X1_train = X[indices1] \n",
    "Y1_train = y[indices1] \n",
    "X2_train = X[indices2]\n",
    "Y2_train = y[indices2]\n",
    "X3_train = X[indices3]\n",
    "Y3_train = y[indices3]\n",
    "\n",
    "X_test = X[indices_test]\n",
    "Y_test = y[indices_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1610124054689,
     "user": {
      "displayName": "Stephanie Valleau",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPxYC_xbIb6qmkXo8iMld6AvpYL5dO54osIaSx=s64",
      "userId": "13099634571785749992"
     },
     "user_tz": 480
    },
    "id": "ngR9-UpO_JHR",
    "outputId": "dee4fd17-2aec-4c08-94a0-cb0f49ce8685"
   },
   "outputs": [],
   "source": [
    "# Let's plot the three training sets and the test set\n",
    "plt.scatter(X1_train, Y1_train, s=100, marker='.', color=\"royalblue\", label=\"training set 1\")\n",
    "plt.scatter(X2_train, Y2_train, s=100, marker='.', color=\"indianred\", label=\"training set 2\")\n",
    "plt.scatter(X3_train, Y3_train, s=100, marker='.', color=\"mediumseagreen\", label=\"training set 3\")\n",
    "plt.scatter(X_test, Y_test, s=100, marker='.', color=\"grey\", label=\"test set 3\")\n",
    "plt.xlabel('time [hours]')\n",
    "plt.ylabel('position [miles]')\n",
    "plt.legend(bbox_to_anchor=(1.8, 1), loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qL3-8SU4QBwb"
   },
   "source": [
    "Now we will carry out linear regression on each of these three training sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxpHTvcKQBwb"
   },
   "outputs": [],
   "source": [
    "regr1 = linear_model.LinearRegression()\n",
    "regr1.fit(X1_train, Y1_train)\n",
    "\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(X2_train, Y2_train)\n",
    "\n",
    "regr3 = linear_model.LinearRegression()\n",
    "regr3.fit(X3_train, Y3_train)\n",
    "\n",
    "# These are simply beta_1 and beta_0 for each fit\n",
    "m1 = regr1.coef_[0][0]\n",
    "b1 = regr1.intercept_[0]\n",
    "\n",
    "m2 = regr2.coef_[0][0]\n",
    "b2 = regr2.intercept_[0]\n",
    "\n",
    "m3 = regr3.coef_[0][0]\n",
    "b3 = regr3.intercept_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaTICfwCQBwb"
   },
   "outputs": [],
   "source": [
    "# Let's see how well they fit/predict the total dataset (note that usually we would only look at the test set here)\n",
    "Y_calc_1 = regr1.predict(X_test)\n",
    "Y_calc_2 = regr2.predict(X_test)\n",
    "Y_calc_3 = regr3.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lo85wEPOQBwb"
   },
   "source": [
    "Let's visualize the fitted values using our trained models and compare to the average fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 563,
     "status": "ok",
     "timestamp": 1610124064476,
     "user": {
      "displayName": "Stephanie Valleau",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPxYC_xbIb6qmkXo8iMld6AvpYL5dO54osIaSx=s64",
      "userId": "13099634571785749992"
     },
     "user_tz": 480
    },
    "id": "JZCSJGtjQBwb",
    "outputId": "052156f4-2b2e-466b-a52b-26e08b11a96a"
   },
   "outputs": [],
   "source": [
    "# Lets plot exact positions respect to the time values using a scatter plot\n",
    "plt.scatter(X_test, Y_test, s=100, marker='o', facecolors='lightgrey', edgecolors='slategrey', label=\"test data\")\n",
    "plt.xlabel('time [hours]')\n",
    "plt.ylabel('position [miles]')\n",
    "\n",
    "# Compute the average calculated position\n",
    "Y_calc_ave =  ( Y_calc_1 + Y_calc_2 + Y_calc_3 ) / 3\n",
    "\n",
    "# Now we compare to our fit by plotting both Y_cal and Y_calc_2 respect to time \n",
    "plt.plot(X_test, Y_calc_1, color='royalblue',linewidth=3, label='linear fit 1')\n",
    "plt.plot(X_test, Y_calc_2, color='indianred',linewidth=3, label='linear fit 2')\n",
    "plt.plot(X_test, Y_calc_3, color='mediumseagreen',linewidth=3, label='linear fit 3')\n",
    "plt.plot(X_test, Y_calc_ave, color='dimgrey',linewidth=3, label='average fit')\n",
    "plt.legend(bbox_to_anchor=(1.8, 1), loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9i5tuSnQBwc"
   },
   "source": [
    "#### Question \n",
    "Why do you think each linear fit has a different slope and intercept?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-m8u65UjQBwc"
   },
   "source": [
    "In the plot we see that each trained model has a different slope and intercept. The average fit is shown in grey. The difference between what one of the trained model predicts and the average model prediction is what we call the **_variance_**.\n",
    "\n",
    "The variance is the amount by which the prediction will change if different subsets of the training data sets are used. We generally want to have a low variance as that ensure that our model is not sensitive to small fluctuations in the dataset.\n",
    "\n",
    "Large variance occurs when the model performs well on the training dataset but does not do well on the test dataset.\n",
    "\n",
    "If we use more complex models to fit, for example polynomials, that will generally lead to a larger variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpNiMOObQBwc"
   },
   "source": [
    "### 4.2 Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOnHzKWFQBwc"
   },
   "source": [
    "Now, let's imagine that we are given the true universal Cheetah position function, lets call it <code> true_cheetah </code> we define it below\n",
    "\n",
    "```\n",
    "def true_cheetah(t):\n",
    "    return 63.64 * t +  155.03\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAhhvCJcQBwc"
   },
   "outputs": [],
   "source": [
    "def true_cheetah(t):\n",
    "    return 63.64 * t +  155.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54BMMnMOQBwc"
   },
   "source": [
    "How well are we doing in terms of our prediction error respect to the true values? Are we biased? Let's visualize our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 429,
     "status": "ok",
     "timestamp": 1610124079203,
     "user": {
      "displayName": "Stephanie Valleau",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPxYC_xbIb6qmkXo8iMld6AvpYL5dO54osIaSx=s64",
      "userId": "13099634571785749992"
     },
     "user_tz": 480
    },
    "id": "RHhbr1THQBwc",
    "outputId": "eddaaa1c-bcfb-4bff-ee24-77bdbd8dc558"
   },
   "outputs": [],
   "source": [
    "plt.plot(X_test, true_cheetah(X_test), color='mediumseagreen',linewidth=3, label='true cheetah motion')\n",
    "plt.plot(X_test, Y_calc_ave, color='dimgrey',linewidth=3, label='average fit')\n",
    "plt.scatter(X_test, Y_test, s=100, marker='o', facecolors='lightgrey', edgecolors='slategrey', label=\"test data\")\n",
    "plt.xlabel('time [hours]')\n",
    "plt.ylabel('position [miles]')\n",
    "plt.legend(bbox_to_anchor=(1.9, 1), loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggJ6fJqOQBwc"
   },
   "source": [
    "We see that our model is biased, indeed our average fit model seems to predict with a **_bias_** error respect to the true cheetah motion. \n",
    "\n",
    "However, if we increased the number of samples over which we averaged, we would eventually get the exact population regression line and exact values of $\\beta_0$ and $\\beta_1$. Indeed ordinary least mean square optimization is **unbiased** - there is no tendency to systematically over or under estimate the true parameters. \n",
    "\n",
    "<div>\n",
    "<img src=\"attachment:cheetah_over_underfitting.png\" align=left width=\"400\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1vW-7Es4ISD"
   },
   "source": [
    "\n",
    "<div>\n",
    "<img src=https://drive.google.com/uc?export=view&id=1DXLgVKjzBrofSPtxtNcVb9Lf_FbUMcKe width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj5_0CMZQBwc"
   },
   "source": [
    "We usually want to find a sweet spot which minimizes both bias and variance. As we increase complexity, for instance by using a polynomial fit, our bias goes down however variance increases. Similarly for a very simple model the bias is large while the variance is small. We can see this graphically in the sketch copied below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pAxYLVq5yHN"
   },
   "source": [
    "<div>\n",
    "<img src=https://drive.google.com/uc?export=view&id=1phnDnjjGJ8twf7h9JKS3TbbN5QlBFj9o width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbXhulIfQBwc"
   },
   "source": [
    "### Next time - Beyond a single input feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tizRPJ_GQBwc"
   },
   "source": [
    "The **speed** or velocity of the Cheetah (the dependent variable v) could depend on:\n",
    "* his/her **energy** (independent variable E) that day\n",
    "* how well the Cheetah **slept** the night before (independent variable sleep, S) \n",
    "* whether she/he was well **fed** (independent variable, F) \n",
    "* how much he or she has been **training** recently (independent variable, T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C-HACK Tutorial 5: Regression and Error for instructors.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (tunnel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
