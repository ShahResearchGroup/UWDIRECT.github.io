{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Methods for Clean Energy Research \n",
    "## _Neural Networks_\n",
    "\n",
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dataset & rescale\n",
    "### 2. Build a NN model with Keras\n",
    "### 3. Train NN and test\n",
    "### 4. Increase complexity of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting libraries\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set larger fontsize for all plots\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the data, splitting into train and test and rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set our random seed so that all computations are deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the raw data for the HCEPDB into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/HCEPDB_moldata.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smaller = df.sample(frac=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smaller.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate out the input features from the output target `'pce'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_smaller[['mass', 'voc', 'jsc', 'e_homo_alpha', 'e_gap_alpha', \n",
    "        'e_lumo_alpha']].values\n",
    "Y = df_smaller[['pce']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the test / train split from this data and keep 20% for testing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pn, X_test_pn, y_train, y_test = train_test_split(X, Y,\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need rescale the input features of the training set using the `StandardScaler()` class [more info](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and store the scaler to use it for the future test sets. We are rescaling input features because we do not want one of the input features to matter more than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the scaler from the training data only and keep it for later use\n",
    "X_train_scaler = StandardScaler().fit(X_train_pn)\n",
    "# apply the scaler transform to the training data\n",
    "X_train = X_train_scaler.transform(X_train_pn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reuse that scaler transform on the test set.  This way we never contaminate the test data with the training data.  We'll start with a histogram of the testing data just to prove to ourselves it is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(10,4))\n",
    "ax[0].hist(X_test_pn[:,0], alpha=0.6, color='mediumvioletred')\n",
    "ax[0].set_xlabel('mass')\n",
    "ax[0].set_ylabel('count')\n",
    "ax[0].set_title('mass distribution',fontsize=18)\n",
    "ax[1].hist(X_test_pn[:,1], alpha=0.6, color='royalblue')\n",
    "ax[1].set_xlabel('voc')\n",
    "ax[1].set_ylabel('count')\n",
    "ax[1].set_title('voc distribution',fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, bnow apply the training scaler transform to the test and plot a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_train_scaler.transform(X_test_pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(10,4))\n",
    "ax[0].hist(X_test[:,0], alpha=0.6, color='mediumvioletred')\n",
    "ax[0].set_xlabel('mass')\n",
    "ax[0].set_ylabel('count')\n",
    "ax[0].set_title('mass distribution',fontsize=18)\n",
    "ax[1].hist(X_test[:,1], alpha=0.6, color='royalblue')\n",
    "ax[1].set_xlabel('voc')\n",
    "ax[1].set_ylabel('count')\n",
    "ax[1].set_title('voc distribution',fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating the neural network model\n",
    "\n",
    "This is a simple neural network with no hidden layers and just the inputs transitioned to the output. Here we will use `Keras` functions [Keras documentation](https://keras.io/guides/). We will use `Dense` layers as defined [here](https://keras.io/api/layers/core_layers/dense/), and the `Adam` [optimizer](https://keras.io/api/optimizers/adam/) which relies on gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_network():\n",
    "    # assemble the structure\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, input_dim=6, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = simple_network()\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the neural network with the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the random number generator - i.e. get reproducible starting weights\n",
    "np.random.seed(seed)\n",
    "# create the NN framework\n",
    "estimator = KerasRegressor(build_fn=simple_network,\n",
    "        epochs=150, batch_size=10000, verbose=0)\n",
    "history = estimator.fit(X_train, y_train, validation_split=0.33, epochs=150, \n",
    "        batch_size=10000, verbose=0, callbacks=TqdmCallback(verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history object returned by the `fit` call contains the information in a fitting run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"final MSE for train is %.2f and for validation is %.2f\" % \n",
    "      (history.history['loss'][-1], history.history['val_loss'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'], '--', c='b')\n",
    "plt.plot(history.history['val_loss'], c='crimson')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the MSE for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = estimator.model.evaluate(X_test, y_test)\n",
    "print(\"test set mse is %.2f\" % test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our train mse is **very similar** to the training and validation at the final step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Let's look at another way to evaluate the set of models using cross validation\n",
    "\n",
    "Use 10 fold cross validation to evaluate the models generated from our training set.  We'll use scikit-learn's tools for this.  Remember, this is only assessing our training set.  If you get negative values, to make `cross_val_score` behave as expected, we have to flip the signs on the results (incompatibility with keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "\n",
    "results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (-1 * results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Increase complexity of NN\n",
    "\n",
    "Let's use a hidden layer this time. Note: it is worthwhile to test how the final loss changes with number of epochs of training as well as learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medium_network():\n",
    "    # assemble the structure\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, input_dim=6, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(12, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.9)\n",
    "    # compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the andom seed as this is used to generate\n",
    "# the starting weights\n",
    "np.random.seed(seed)\n",
    "# create the NN framework\n",
    "estimator = KerasRegressor(build_fn=medium_network,\n",
    "        epochs=150, batch_size=10000, verbose=0)\n",
    "history = estimator.fit(X_train, y_train, validation_split=0.30, epochs=150, \n",
    "        batch_size=10000, verbose=0, callbacks=TqdmCallback(verbose=0))\n",
    "print(\"Final MSE for train is %.3e and for validation is %.3e\" % \n",
    "      (history.history['loss'][-1], history.history['val_loss'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'],'--',c='b')\n",
    "plt.plot(history.history['val_loss'],c='crimson')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim([6,8])\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = estimator.model.evaluate(X_test, y_test)\n",
    "print(\"test set mse is %.3e\" % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medium_network(lr=0.8):\n",
    "    # assemble the structure\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, input_dim=6, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(12, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    # compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the andom seed as this is used to generate\n",
    "# the starting weights\n",
    "np.random.seed(seed)\n",
    "# create the NN framework\n",
    "MSE = []\n",
    "lr_vals = [0.00001, 0.0001, 0.01, 0.1, 1.0]\n",
    "for l in lr_vals:\n",
    "    estimator = KerasRegressor(build_fn=medium_network, \n",
    "                               epochs=150, batch_size=10000, \n",
    "                               verbose=0, lr=l)\n",
    "    history = estimator.fit(X_train, y_train, validation_split=0.30,\n",
    "                            epochs=150, batch_size=10000, \n",
    "                            verbose=0, callbacks=TqdmCallback(verbose=0))\n",
    "    print(\"Final MSE for train is %.3e and for validation is %.3e\" % \n",
    "      (history.history['loss'][-1], history.history['val_loss'][-1]))\n",
    "    MSE.append([history.history['loss'][-1], \n",
    "                history.history['val_loss'][-1],\n",
    "                estimator.model.evaluate(X_test, y_test)])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lr_vals, [m[0] for m in MSE],'-o',lw=3,label='train')\n",
    "plt.plot(lr_vals, [m[1] for m in MSE],'-.s',lw=3, label='validation')\n",
    "plt.plot(lr_vals, [m[2] for m in MSE],':>',lw=3, label='test')  \n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tunnel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
