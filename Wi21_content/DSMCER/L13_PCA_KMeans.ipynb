{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Methods for Clean Energy Research \n",
    "## _Unsupervised Machine Learning_\n",
    "\n",
    "## Outline\n",
    "\n",
    "### 1. Principal Component Analysis\n",
    "### 2. K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "\n",
    "import pandas as pd \n",
    "import random\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Principal Component Analysis\n",
    "\n",
    "### 1.1 What is PCA?\n",
    "\n",
    "PCA finds a low-dimensional representation of a high-dimensional dataset \n",
    "\n",
    "$$X = \\left\\{X_1,X_2,...,X_p\\right\\}$$\n",
    "\n",
    "which contains as much of the information as possible on the high-dimensional dataset. \n",
    "\n",
    "The low-dimensional representation will have dimensions $n<p$ where $p$ the number of input features also represents the dimensions of the dataset to some extent.\n",
    "\n",
    "* Each of the $n$ low-dimensional components can be written as a linear combination of the input features\n",
    "* All low-dimensional components are uncorrelated which also means, in this case, that they are orthogonal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some **applications of PCA:**\n",
    "\n",
    "* Reducing dimensionality\n",
    "* Accelerating ML methods ( finding optimal features )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **first** principal component of our set of features $X = \\left\\{X_1,X_2,...,X_p\\right\\}$ is the linear combination\n",
    "\n",
    "$$Z_1 = \\phi_{11}X_1 \\,+\\, \\phi_{21}X_2 \\,+\\, ...\\,+\\,\\phi_{p1}X_p$$\n",
    "\n",
    "which has the largest variance. The coefficients $\\left\\{\\phi_{1,i}\\right\\}_{i=1}^{p}$ are called _loadings_ of the first principal component. The _loadings_ are such that $Z_1$ is normalized, i.e. $\\sum_i^p\\phi_{i,1}^2=1$\n",
    "\n",
    "The **second** principal component is the direction which maximizes variance among all directions orthogonal to the first. \n",
    "\n",
    "The $k$-th component is the variance-maximizing direction orthogonal to the previous $k − 1$ components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The PCA optimization problem\n",
    "\n",
    "**How do we find these _loadings_** with the constraint of largest variance?\n",
    "\n",
    "Let's say you are given a data set $X$ of $n$ examples, i.e. of size $n\\times p$, and want to find\n",
    "\n",
    "$$z_{i1} = \\phi_{11}x_{i1} + \\phi_{21}x_{i2} + ...\\,+\\,\\phi_{p1}x_{ip}\\;\\;\\text{with } i\\in\\left[1,n\\right]$$\n",
    "\n",
    "that has the largest sample variance.\n",
    "\n",
    "One can show, e.g for the first component that the solution is to solve the optimization problem\n",
    "\n",
    "$$\\underset{\\phi_{11},...\\phi_{p1}}{\\text{maximize}}\\left\\{\\frac{1}{n}\\sum_{j=1}^{n}{z_{j1}^2}\\right\\}\\;\\text{ subject to}\\sum_i^p\\phi_{i,1}^2=1$$\n",
    "\n",
    "the process is similar for all subsequent components. \n",
    "\n",
    "**Note** that the above equation assumes that the means of each $X_i$ is **equal to zero**. \n",
    "\n",
    "Also, we usually **rescale** the $X_i$ to have a variance equal to 1. This insures that no feature will dominate over another due to a difference in range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 How to implement PCA using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "X = np.dot(rng.rand(3, 3)*10, rng.randn(3, 200)*2).T\n",
    "\n",
    "# Note we have three input features but are only going to plot the data\n",
    "# as a function of the first two\n",
    "plt.scatter(X[:, 0], X[:, 1],color='darkseagreen')\n",
    "plt.xlabel('$X_1$')\n",
    "plt.ylabel('$X_2$')\n",
    "plt.legend(['Dataset'])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement the PCA algorithm and search for $n=3$ principal components which are orthogonal and best represent our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca.fit(X)\n",
    "\n",
    "print(\"PCA components:\\n\", pca.components_)\n",
    "print(\"PCA proportion of variance explained:\\n\", pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In class exercise** - try running the above for `n_components=2` do they change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Proportion of Variance explained\n",
    "\n",
    "What is the **proportion of variance explained (PVE)**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (5 min breakout room):\n",
    "\n",
    "Create a plot of the explained variance as a function of the principal components. What trend do you see? What does it show you about your principal components? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PVE tells us how much of the total variance in the $X$ data is explained by each component! The larger it is the more it describes the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here it is clear that we might be ok with simply taking two input features to represent our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to two input features and look at the PCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating new data\n",
    "rng = np.random.RandomState(seed=42)\n",
    "X_2 = np.dot(rng.rand(2, 2)*10, rng.randn(2, 200)*2).T\n",
    "\n",
    "# Calling PCA \n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_2.fit(X_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to draw two vectors\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, color='purple')\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "plt.scatter(X_2[:, 0], X_2[:, 1], alpha=0.2, color='mediumorchid')\n",
    "plt.xlabel('$X_1$')\n",
    "plt.ylabel('$X_2$')\n",
    "for length, vector in zip(pca_2.explained_variance_, pca_2.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca_2.mean_, pca_2.mean_ + v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `explained_variance_ratio_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA can become **expensive to calculate**, especially as the data set size grows \n",
    "\n",
    "**Advice:**\n",
    "Go slow and use a subset of your data (if you have many points) \n",
    "Use PCA as a guide and as an exploratory tool \n",
    "Constantly interrogate the results and ask if they make sense! You don’t have “test set error” to fall back on, so you need to use your brain!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The K-means method and algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While PCA searches for a low dimensional representation of the dataset, Clustering methods search for homogeneous subgroups among the observations.\n",
    "\n",
    "In K-means clustering, we define a desired number of clusters _K_ and assign each observation to one of the clusters.\n",
    "\n",
    "**Rules:** \n",
    "* Each observation must be placed in at least one of the clusters\n",
    "* No clusters may overlap, each observation can only be placed in a single cluster \n",
    "* The goal is to minimize the variance of observations within each of the clusters \n",
    "\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "Choose a values for your number of clusters _K_:\n",
    "\n",
    "   * At random select _K_ points from your dataset and assign those to be _centroids_, cluster centers\n",
    "   * Assign each data point to the closest centroid (use the Euclidean distance)\n",
    "   * Re-compute the centroids using the currently assigned clusters\n",
    "   * If a convergence criterion is not met, repeat steps 2 and  3\n",
    "\n",
    "This algorithm corresponds to solving the problem\n",
    "\n",
    "$$\\underset{C_{1},...,C_{K}}{\\text{minimize}}\\sum_{j=1}^K\\sum_{{\\bf x}\\in {\\bf C}_j}d({\\bf x},{\\bf m}_j)^2$$\n",
    "\n",
    "with \n",
    "\n",
    "$d({\\bf x},{\\bf m}_j)$ the Euclidean distance between data point x\n",
    "and centroid $m_j$, $C_j$ the $j-$th cluster, and $m_j$\n",
    "is the centroid of cluster $C_j$ i.e. the mean vector of all the\n",
    "data points in $C_j$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 K-means from scratch\n",
    "\n",
    "Let's implement it from scratch (following this [tutorial](https://towardsdatascience.com/a-complete-k-mean-clustering-algorithm-from-scratch-in-python-step-by-step-guide-1eb05cdcd461))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will generate data, note that while we are generating y - the algorithm will never see it or know of its existence (unsupervised!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_features=2,\n",
    "     n_samples=1000,\n",
    "     centers=3,\n",
    "     cluster_std=4.0,\n",
    "     random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we randomly select some examples in X as our centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_centroid_indices = random.sample(range(0, len(X)), 3)\n",
    "\n",
    "print(init_centroid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = []\n",
    "for i in init_centroid_indices:\n",
    "    centroids.append(X[i])\n",
    "centroids = np.array(centroids)\n",
    "\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function to compute the Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eucl_dist(X1, X2):\n",
    "    return np.sqrt(sum((X1 - X2)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (7 min breakout room)\n",
    "Complete the `assigned_centroids` function below. It aims to assign the centroid each point X belongs to, based on its distance the points in each centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_centroids(centroids, X):\n",
    "    assigned_centroids = []\n",
    "    for xi in X:\n",
    "        distance = []\n",
    "        for Cj in centroids:\n",
    "            # Complete here - fill out the distances\n",
    "        # From the distances choose the assigned centrois\n",
    "    return assigned_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_centroids = assign_centroids(centroids, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last function to implement is one to update the centroids based on the mean for features in each assigned centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_centroids(clusters, X):\n",
    "    new_centroids = []\n",
    "    df = pd.concat([pd.DataFrame(X), pd.DataFrame(clusters, columns=['cluster'])], axis=1)\n",
    "    for c in set(df['cluster']):\n",
    "        current_cluster = df[df['cluster'] == c][df.columns[:-1]]\n",
    "        cluster_mean = current_cluster.mean(axis=0)\n",
    "        new_centroids.append(cluster_mean)\n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this looks like in a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "for i in range(10):\n",
    "    assigned_centroids = assign_centroids(centroids, X)\n",
    "    centroids = update_centroids(assigned_centroids, X)\n",
    "    print(centroids)\n",
    "    pl.clf()\n",
    "    pl.scatter(X[:, 0], X[:, 1], alpha=0.3, c=y, s=50, cmap='coolwarm')#color='cornflowerblue')\n",
    "    pl.scatter(np.array(centroids)[:, 0], \n",
    "               np.array(centroids)[:, 1], \n",
    "               marker='>', color='black', s=80)\n",
    "    pl.title('Iteration '+str(i))\n",
    "    pl.xlabel('$X_1$', fontsize=20)\n",
    "    pl.ylabel('$X_2$', fontsize=20)\n",
    "    display.display(pl.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(1.0)\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Exercise (10 min breakout room): K-Means with `sklearn` \n",
    "\n",
    "Now let's try the same thing with the sklearn `KMeans` [class](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (tunnel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
