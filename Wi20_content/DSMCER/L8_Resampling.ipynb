{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we are going to be examining two tools that I think border on statistical magic called **bootstrapping** and **k-fold cross-validation**. We have seen bootstrapping before when we introduced descriptive statistics, so maybe we will start there for today.\n",
    "\n",
    "Recall the difference between a **population** and a **sample**. The population is all the possible observations out there. For instance if I were an epidemiologist, this might be people in the U.S. or children in Washington state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img width=\"304\" height=\"232\" alt=\"Image result for population versus sample\" src=\"http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_BiostatisticsBasics/Sampling3.jpg\">\n",
    "\n",
    "But this can also be applied to measurements in the lab as well such as possible voltage values from this battery. There could be an infinite number of measurements, but either way: accessing the actual population is virtually impossible.\n",
    "\n",
    "<img width=\"304\" height=\"223\" alt=\"Image result for thermometer gif\" src=\"https://media.giphy.com/media/26FL3uMhARSAvIZZS/giphy.gif\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a scientist, you only have access to a sample. Part of designing an experiment is choosing how big your sample should be.\n",
    "\n",
    "But a key problem is: if you change your sample, it could change your sample mean and sample variance significantly. We talked previously about how calculating **p values** is one way of establishing confidence in our measurements. But **bootstrapping** is a fancy statistical way of getting uncertainties (such as confidence intervals and standard errors in linear regression) just by playing with your sampling.\n",
    "\n",
    "Let's demonstrate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we were in the lab, and we were making a calibration curve, say absorbance against concentration using UV-vis measurements. You generate the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data and do linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.linspace(0, 100, 20)\n",
    "x_raw = np.linspace(0, 4, 20)\n",
    "x = x_raw + np.exp(x_raw*0.4*np.random.rand(20)) - 1\n",
    "\n",
    "slope, intercept, rvalue, pvalue, stderr = stats.linregress(x, y)\n",
    "y_fit = intercept + np.array([0,8])*slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "ax.scatter(x, y, alpha=0.5, label='raw data')\n",
    "ax.plot(np.array([0,8]), y_fit, linewidth=4, label='linear fit')\n",
    "#ax.set_yscale('log')\n",
    "#ax.set_xscale('log')\n",
    "ax.set_xlim([0, 8])\n",
    "ax.set_ylim([0, 120])\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But uh oh. You can see that some are those points are a little wiggly. You want to get some indication of how your calibration curve would change if you repeated the experiment again. Instead of actually repeating the experiment, you decide to use bootstrapping.\n",
    "\n",
    "In bootstrapping, you will resample from you original dataset (pretending its representative of the actual population) with replacement and perform a regression on this sub-sample of your sample. We will generate some code to do this for us. We will take advantage of `pandas` sampling capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'x': x, 'y':y})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.scatter(x, y, alpha=0.5, label='raw data')\n",
    "\n",
    "# initializing outputs\n",
    "slope = np.zeros(10)\n",
    "intercept = np.zeros(10)\n",
    "rvalue = np.zeros(10)\n",
    "pvalue = np.zeros(10)\n",
    "stderr = np.zeros(10)\n",
    "\n",
    "for i in range(0, 10):\n",
    "    # Using pandas to grab a random sample WITH replacement\n",
    "    subsample = data.sample(n=20, replace=True)\n",
    "    #print(i)\n",
    "    # Linear regression performed on each sample.\n",
    "    slope[i], intercept[i], rvalue[i], pvalue[i], stderr[i] = stats.linregress(subsample['x'], subsample['y'])\n",
    "    y_fit = intercept[i] + np.array([0,8])*slope[i]\n",
    "\n",
    "    ax.plot(np.array([0,8]), y_fit, linewidth=2, color='b', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat this, like, 1000 times, generate a distribution of slopes and intercepts, and use the distributions to calculate confidence intervals on the slope and intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.scatter(x, y, alpha=0.5, label='raw data')\n",
    "n=1000\n",
    "\n",
    "# initializing outputs\n",
    "slope = np.zeros(n)\n",
    "intercept = np.zeros(n)\n",
    "rvalue = np.zeros(n)\n",
    "pvalue = np.zeros(n)\n",
    "stderr = np.zeros(n)\n",
    "\n",
    "for i in range(0, n):\n",
    "    # Using pandas to grab a random sample WITH replacement\n",
    "    subsample = data.sample(n=20, replace=True)\n",
    "    #print(i)\n",
    "    # Linear regression performed on each sample.\n",
    "    slope[i], intercept[i], rvalue[i], pvalue[i], stderr[i] = stats.linregress(subsample['x'], subsample['y'])\n",
    "    y_fit = intercept[i] + np.array([0,8])*slope[i]\n",
    "\n",
    "    ax.plot(np.array([0,8]), y_fit, linewidth=2, color='b', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('slope: {:.2f} (95% CI: {:.2f} - {:.2f})'.format(np.mean(slope),\n",
    "                                                       np.quantile(slope, 0.025),\n",
    "                                                       np.quantile(slope, 0.975)))\n",
    "\n",
    "print('intercept: {:.2f} (95% CI: {:.2f} - {:.2f})'.format(np.mean(intercept),\n",
    "                                                       np.quantile(intercept, 0.025),\n",
    "                                                       np.quantile(intercept, 0.975)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a note, this isn't the most useful example, as `stats.linregress` actually calculates SEM values for you. But bootstrapping is generalizable to any statistical quantities you may be interested in. One useful example is error propagation. If you know the error in raw datasets, and you want to know how that error will impact downstream calculations, you can use bootstrapping to do it without having to do the calculus involved in standard error propagation.\n",
    "\n",
    "Similarly, bootstrapping can be used to estimate error in machine learning methods extending beyond simple linear regression, but we we'll leave it at that so we can move on to other topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation is similar, but, well, different. Cross validation is related to sampling, just like bootstrapping, but is used in slightly different contexts. \n",
    "\n",
    "Remember when we talked about the difference between training and test datasets?\n",
    "\n",
    "<img width=\"304\" height=\"175\" alt=\"Image result for training test dataset\" src=\"http://scott.fortmann-roe.com/docs/docs/MeasuringError/holdout.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When **training** a machine learning model, we split our dataset into a **training dataset** and a **test dataset**. The training dataset is used to train our machine learning model. The test dataset is used to verify the accuracy of our trained model. It is **very** important that the test dataset and training dataset are completely separate, otherwise you could overestimate the accuracy of your model.\n",
    "\n",
    "We rarely have an indepedent test dataset. In practice, we will randomly split our sample dataset into a training dataset and test dataset. This process is called **validation**. Validation is typically used in two scenarios: **model selection** (proper level of flexibility) and **model assessment** (evaluating a model's performance). We will take a look at both use cases.\n",
    "\n",
    "`scikit-learn` handily has a function that does this split for us. But first, let's create a datset that we can practice on using a regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 1000)\n",
    "y = 0.2*x + 1*x**2 - 0.3*x**3 + 0.021*x**4 + 0.3*np.random.rand(1000)*x\n",
    "\n",
    "plt.scatter(x, y, alpha=0.1)\n",
    "\n",
    "X = pd.DataFrame({'x':x, 'x2':x**2, 'x3':x**3, 'x4':x**4, 'x5':x**5, 'x6':x**6, 'x7':x**7, 'x8':x**8,\n",
    "                  'x9':x**9, 'x10':x**10, 'x11':x**12, 'x13':x**13, 'x14':x**14, 'x15':x**15, 'x16':x**16, 'x17':x**17})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use validation to select the degree of flexibility of our linear model. First, the split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_train[['x']], y_train)\n",
    "y_pred = reg.predict(X_test[['x']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, alpha=0.1)\n",
    "plt.plot(X_test[['x']], y_pred, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a measure of the degree of quality of fit using the mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's repeat the linear regression while increasing the degree of our polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.scatter(x, y, alpha=0.1)\n",
    "mse = np.zeros(17)\n",
    "\n",
    "for i in range(1, 17):\n",
    "    reg = LinearRegression().fit(X_train[X_train.columns[0:i].values], y_train)\n",
    "    y_pred = reg.predict(X_test[X_test.columns[0:i].values])\n",
    "\n",
    "    ax.scatter(X_test[['x']], y_pred, marker='o', s=25, label=i)\n",
    "    mse[i-1] = mean_squared_error(y_test, y_pred)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.plot(np.linspace(1, 16, 17), mse, linewidth=4)\n",
    "ax.set_ylabel('Mean Squared Error')\n",
    "ax.set_xlabel('Degree of Polynomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, we can see that the MSE reaches a local minimum at a degree of 4. This seems to be the best balance between bias and variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This validation method has two disadvantages:\n",
    "    \n",
    "* The test error is highly variable, depending on your training dataset. If we used a different training dataset, we'd have a different error\n",
    "* We only use a subset of the training dataset to create our model. The fewer the observations, the worse the final model (we actually overestimate the test error rate!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "for run in range(8):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "    mse = np.zeros(17)\n",
    "\n",
    "    for i in range(1, 18):\n",
    "        reg = LinearRegression().fit(X_train[X_train.columns[0:i].values], y_train)\n",
    "        y_pred = reg.predict(X_test[X_test.columns[0:i].values])\n",
    "\n",
    "        mse[i-1] = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    \n",
    "    ax.plot(np.linspace(1, 16, 17), mse, linewidth=4, color='b', alpha=0.2)\n",
    "    ax.set_ylabel('Mean Squared Error')\n",
    "    ax.set_xlabel('Degree of Polynomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common alternative to is **K-fold cross-validation**. In this configuration, we split the original dataset into *k* units. In on training-test configuration, *k-1* of the units will be used as a training dataset, while the remaining unit will be used as a test dataset. We train a model *k* times, switching out the test dataset each time, and use the average error as an estimate of the actual test error.\n",
    "\n",
    "<img width=\"608\" height=\"334\" alt=\"Image result for k-fold cross-validation\" src=\"https://cdn-images-1.medium.com/max/1600/1*me-aJdjnt3ivwAurYkB7PA.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "kf.get_n_splits(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "mse = np.zeros((17, 10))\n",
    "j = 0\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    for i in range(1, 18):\n",
    "        reg = LinearRegression().fit(X_train[X_train.columns[0:i].values], y_train)\n",
    "        y_pred = reg.predict(X_test[X_test.columns[0:i].values])\n",
    "\n",
    "        mse[i-1, j] = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    \n",
    "    ax.plot(np.linspace(1, 16, 17), mse[:, j], linewidth=4, color='b', alpha=0.09)\n",
    "    ax.set_ylabel('Mean Squared Error')\n",
    "    ax.set_xlabel('Degree of Polynomial')\n",
    "    j += 1\n",
    "    \n",
    "avg_mse = mse.mean(axis=1)\n",
    "ax.plot(np.linspace(1, 16, 17), avg_mse, color='purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
