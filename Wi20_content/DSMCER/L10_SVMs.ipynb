{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tried some basic **classification** and **regression** algorithms, more complicated algorithms will follow a familiar pattern: given a set of inputs and outputs, can I develop a model that relates the inputs to the outputs, and can I use that model to predict new outputs from a set of new inputs?\n",
    "\n",
    "In this sense, you could quite easily treat these new regression and classification techniques as black boxes, feed data in and get data out. But don't do that. For a number of reasons. First, that's boring, right? And second, if you don't understand the inner workings of each classifier and regressor, you won't know how to properly tune them (a part of model selection). Here's one example from `scikit-learn` comparing a wide variety of classification techniques:\n",
    "\n",
    "<img alt=\"sklearn_classification.png\" src=\"https://github.com/UWDIRECT/UWDIRECT.github.io/blob/master/Wi19_content/DSMCER/images/sklearn_classification.png?raw=true\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's get some data. For the purposes of this notebook, we're going to create a linearly separable dataset really quick using some `scikit-learn magic`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1, class_sep=0.7)\n",
    "#rng = np.random.RandomState(2)\n",
    "#X += 2 * rng.uniform(size=X.shape)\n",
    "data = (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a training-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data to see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(figsize=(4,4))\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "# Plot the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "           edgecolors='k', label='Training')\n",
    "# Plot the testing points\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "           edgecolors='k', label='Test')\n",
    "#ax.set_xlim([-3,4])\n",
    "#ax.set_ylim([0,4])\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember how our KNN classifier worked? It was able to predict the class based on the surrounding K nearest neighbors. An SVM works on a different paradigm. In this case, we're going to try to draw a line that splits the two groups. Let's try applying a simple SVM algorithm to do this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear', C=1)\n",
    "\n",
    "h=0.02\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(4,4))\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "if hasattr(clf, \"decision_function\"):\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "else:\n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "Zbi = Z > np.median(Z)\n",
    "ax.contourf(xx, yy, Zbi, cmap=cm, alpha=.1)\n",
    "\n",
    "# Plot the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "           edgecolors='k', label='Training')\n",
    "# Plot the testing points\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "           edgecolors='k', alpha=0.6, label='Test')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, this dataset has two distinct groups, so separating the two with a line is pretty simple. However, that's not always the case. This method would fall apart if there was any overlap. This simple classifier is called the **maximal margin classifier**.\n",
    "\n",
    "What happens when there is no easy boundary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1, class_sep=0.8)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "data = (X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(figsize=(4,4))\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "# Plot the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "           edgecolors='k', label='Training')\n",
    "# Plot the testing points\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "           edgecolors='k', label='Test')\n",
    "#ax.set_xlim([-3,4])\n",
    "#ax.set_ylim([0,4])\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **maximal margin classifier** can't solve this problem anymore. We're now going to introduce a tuning parameter, **C**. Think of C as the \"budget\" for the number of violations we will allow to the decision boundary. When C is zero, you won't tolerate any violations: all blues must be on one side of the line, and all reds must be on the other.\n",
    "\n",
    "In this sense, C, is our **bias/variance tradeoff parameter**. Cool, huh?! Similar to K in K-nearest neighbors. Let's try seeing what happens when we adjust C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = [0.00000001, 0.001, 0.1, 1, 10, 100, 1000, 10000]\n",
    "figure, axes = plt.subplots(nrows=2, ncols=4, figsize=(12, 9))\n",
    "\n",
    "axes = [item for sublist in axes for item in sublist]\n",
    "\n",
    "h=0.02\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "for C, ax in zip(C_values, axes):\n",
    "    clf = SVC(kernel='linear', C=C)\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    Zbi = Z > np.median(Z)\n",
    "    ax.contourf(xx, yy, Zbi, cmap=cm, alpha=.1)\n",
    "\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k', label='Training')\n",
    "    # Plot the testing points\n",
    "#     ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "#                edgecolors='k', alpha=0.6, label='Test')\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_title('C Value: {}'.format(str(C)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, you might be complaining, this isn't even that exciting! Sure, our predictor works OK-ish. But this is literally just drawing a line. Our KNN predictor did better than this! Is this even a useful method?\n",
    "\n",
    "**SVMs** really get their power when you take advantage of something called a **kernel**. A kernal takes your original dataset, transoforms it into higher dimensional space, and then draws a hyperplane in that new hyperdimensional space. Such hyperplane in the original predictor space becomes nonlinear. It's a lot of math that we won't go into here. But let's see what happens when we try to classify some data with a radial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "data = make_circles(noise=0.2, factor=0.5, random_state=1)\n",
    "X, y = data\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear', C=1)\n",
    "\n",
    "h=0.02\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(4,4))\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "if hasattr(clf, \"decision_function\"):\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "else:\n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "Zbi = Z > np.median(Z)\n",
    "ax.contourf(xx, yy, Zbi, cmap=cm, alpha=.1)\n",
    "\n",
    "# Plot the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "           edgecolors='k', label='Training')\n",
    "# Plot the testing points\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "           edgecolors='k', alpha=0.6, label='Test')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, a linear separator isn't going to work here. Let's try a **radial kernel**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma=4, C=0.1)\n",
    "\n",
    "h=0.02\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(4,4))\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "if hasattr(clf, \"decision_function\"):\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "else:\n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "Zbi = Z > np.median(Z)\n",
    "ax.contourf(xx, yy, Zbi, cmap=cm, alpha=.1)\n",
    "\n",
    "# Plot the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "           edgecolors='k', label='Training')\n",
    "# Plot the testing points\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "           edgecolors='k', alpha=0.6, label='Test')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are additional kernels that you can try. If you want to build your SVM skills, try some of these additional exercises:\n",
    "\n",
    "* Calculate the MSE on the above training and test datasets and compare each of the models.\n",
    "* Look up the types of kernels available, and try using some alternate kernels.\n",
    "* Compare the KNN classifier to the linear and radial SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we apply these knowledge to the classification problem of atoms ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: borrow what you learned from the KNN jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "%matplotlib inline\n",
    "# Add a line to import the SVC/SVM pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the data\n",
    "* elemental data: [https://raw.githubusercontent.com/UWDIRECT/UWDIRECT.github.io/master/Wi18_content/DSMCER/atomsradii.csv](https://raw.githubusercontent.com/UWDIRECT/UWDIRECT.github.io/master/Wi18_content/DSMCER/atomsradii.csv)\n",
    "* testing data: [https://raw.githubusercontent.com/UWDIRECT/UWDIRECT.github.io/master/Wi18_content/DSMCER/testing.csv](https://raw.githubusercontent.com/UWDIRECT/UWDIRECT.github.io/master/Wi18_content/DSMCER/testing.csv)\n",
    "\n",
    "From this article in [Scientific Reports](http://www.nature.com/articles/srep13285)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's make a new classifier object\n",
    "\n",
    "Start with  [LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can use the following function to see how your model is doing: \n",
    "\n",
    "#### You and your partner should determine:  \n",
    "* Testing error rate\n",
    "* Training error rate \n",
    "\n",
    "Grab your code from the L9.MLIntro notebook!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With remaining time go through the cell below and look at graphs of the decision boundary vs K. \n",
    "* See if you can use the graph to determine your **testing** error rate  \n",
    "* Could you also use the graph to determine your **training** error rate? (_open ended_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is code to visualize the decision boundary.  Fix it up to use your classifier from above or better yet, try a nonlinear kernel and visualize that!  Name your classifier `clf.predict` and this should just run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional library we will use \n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# just for convenience and similarity with sklearn tutorial\n",
    "# I am going to assign our X and Y data to specific vectors\n",
    "# this is not strictly needed and you could use elements df for the whole thing!\n",
    "\n",
    "X=elements[['rWC','rCh']]\n",
    "\n",
    "#this is a trick to turn our strings (type of element / class) into unique \n",
    "#numbers.  Play with this in a separate cell and make sure you know wth is \n",
    "#going on!\n",
    "levels,labels=pd.factorize(elements.Type)\n",
    "y=levels\n",
    "\n",
    "#This determines levelspacing for our color map and the colors themselves\n",
    "h=0.02\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "# in the sklearn tutorial two different weights are compared\n",
    "# the decision between \"uniform\" and \"distance\" determines the probability\n",
    "# weight.  \"uniform\" is the version presented in class, you can change to \n",
    "# distance\n",
    "weights='uniform'\n",
    "\n",
    "\n",
    "# Straight from the tutorial - quickly read and see if you know what these \n",
    "# things are going - if you are < 5 min until end then you should skip this part \n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = elements.rWC.min() - 0.1  , elements.rWC.max() + 0.1\n",
    "y_min, y_max = elements.rCh.min() - 0.1  , elements.rCh.max() + 0.1  \n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), \n",
    "                     np.arange(y_min, y_max, h)) \n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(4,4));\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "# This may be the 1st time you have seen how to color points by a 3rd vector\n",
    "# In this case y ( see c=y in below statement ). This is very useful! \n",
    "plt.scatter(X.rWC, X.rCh, c=y, cmap=cmap_bold)\n",
    "\n",
    "# Set limits and lebels \n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xlabel('rWC')\n",
    "plt.ylabel('rCh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
